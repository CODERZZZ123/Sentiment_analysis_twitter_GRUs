{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import random as rnd\n",
    "\n",
    "1\n",
    "# import relevant libraries\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import fastmath\n",
    "from trax import shapes\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import Layer from the utils.py file\n",
    "from utils import Layer, load_tweets, process_tweet\n",
    "# import w1_unittest\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'gpu')\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT EDIT THIS CELL\n",
    "\n",
    "\n",
    "# Import functions from the utils.py file\n",
    "\n",
    "def train_val_split():\n",
    "\n",
    "#############Tweeter############################\n",
    "    # Load positive and negative tweets\n",
    "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "    # View the total number of positive and negative tweets.\n",
    "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "    # Split positive set into validation and training\n",
    "    val_pos_tweeter   = all_positive_tweets[4500:] # generating validation set for positive tweets\n",
    "    train_pos_tweeter  = all_positive_tweets[:4500]# generating training set for positive tweets\n",
    "\n",
    "    # Split negative set into validation and training\n",
    "    val_neg_tweeter   = all_negative_tweets[4500:] # generating validation set for negative tweets\n",
    "    train_neg_tweeter  = all_negative_tweets[:4500] # generating training set for nagative tweets\n",
    "    \n",
    "\n",
    "    X_train_tweeter = train_pos_tweeter + train_neg_tweeter\n",
    "    X_val_tweeter  = val_pos_tweeter + val_neg_tweeter\n",
    "    y_train_tweeter = np.append(np.ones(len(train_pos_tweeter)), np.zeros(len(train_neg_tweeter)))\n",
    "    y_val_tweeter  = np.append(np.ones(len(val_pos_tweeter)), np.zeros(len(val_neg_tweeter)))\n",
    "\n",
    "############IMDB################################\n",
    "\n",
    "    # # Load your dataset from the CSV file\n",
    "    # df = pd.read_csv('imdb_dataset_review/IMDB Dataset.csv')\n",
    "\n",
    "    # # Assuming 'sentiment' column contains 'positive' or 'negative'\n",
    "    # positive_reviews_imdb = df[df['sentiment'] == 'positive']\n",
    "    # negative_reviews_imdb = df[df['sentiment'] == 'negative']\n",
    "\n",
    "    # # Split positive reviews into train and validation sets\n",
    "    # train_pos_imdb, val_pos_imdb = train_test_split(positive_reviews_imdb, test_size=0.1, random_state=42)\n",
    "\n",
    "    # # Split negative reviews into train and validation sets\n",
    "    # train_neg_imdb, val_neg_imdb = train_test_split(negative_reviews_imdb, test_size=0.1, random_state=42)\n",
    "\n",
    "    # train_dum_pos_imdb = train_pos_imdb['review'].tolist()\n",
    "    # val_dum_pos_imdb = val_pos_imdb['review'].tolist()\n",
    "    # train_dum_neg_imdb = train_neg_imdb['review'].tolist()\n",
    "    # val_dum_neg_imdb = val_neg_imdb['review'].tolist()\n",
    "    \n",
    "\n",
    "    # # Combine train and validation sets for both positive and negative sentiments\n",
    "    # train_data_imdb = pd.concat([train_pos_imdb, train_neg_imdb])\n",
    "    # val_data_imdb = pd.concat([val_pos_imdb, val_neg_imdb])\n",
    "\n",
    "    # X_train_imdb = train_data_imdb['review'].tolist()\n",
    "    # X_val_imdb = val_data_imdb['review'].tolist()\n",
    "    # y_train_imdb = np.where(train_data_imdb['sentiment'].values == 'positive', 1, 0)\n",
    "    # y_val_imdb = np.where(val_data_imdb['sentiment'].values == 'positive', 1, 0)\n",
    "\n",
    "\n",
    "############USAIR###############################\n",
    "    # df = pd.read_csv('us_airlines_data/Tweets.csv')\n",
    "\n",
    "    # # Assuming 'sentiment' column contains 'positive' or 'negative'\n",
    "    # positive_reviews_usair = df[df['airline_sentiment'].isin(['positive', 'neutral'])]\n",
    "    # negative_reviews_usair = df[df['airline_sentiment'] == 'negative']\n",
    "\n",
    "    # # Split positive reviews into train and validation sets\n",
    "    # train_pos_usair, val_pos_usair = train_test_split(positive_reviews_usair, test_size=0.1, random_state=42)\n",
    "\n",
    "    # # Split negative reviews into train and validation sets\n",
    "    # train_neg_usair, val_neg_usair = train_test_split(negative_reviews_usair, test_size=0.1, random_state=42)\n",
    "\n",
    "    # train_dum_pos_usair = train_pos_usair['text'].tolist()\n",
    "    # val_dum_pos_usair = val_pos_usair['text'].tolist()\n",
    "    # train_dum_neg_usair = train_neg_usair['text'].tolist()\n",
    "    # val_dum_neg_usair = val_neg_usair['text'].tolist()\n",
    "\n",
    "    # # Combine train and validation sets for both positive and negative sentiments\n",
    "    # train_data_usair = pd.concat([train_pos_usair, train_neg_usair])\n",
    "    # val_data_usair = pd.concat([val_pos_usair, val_neg_usair])\n",
    "\n",
    "    # X_train_usair = train_data_usair['text'].tolist()\n",
    "    # X_val_usair = val_data_usair['text'].tolist()\n",
    "    # y_train_usair = np.where(train_data_usair['airline_sentiment'].values == 'negative' , 0, 1)\n",
    "    # y_val_usair = np.where(val_data_usair['airline_sentiment'].values == 'negative', 0, 1)\n",
    "\n",
    "    train_pos = train_pos_tweeter  \n",
    "    train_neg = train_neg_tweeter  \n",
    "\n",
    "    # Combine all reviews and targets for training\n",
    "    train_x = X_train_tweeter \n",
    "    train_y = np.concatenate([y_train_tweeter])\n",
    "\n",
    "    # Combine all positive and negative reviews for validation\n",
    "    val_pos = val_pos_tweeter \n",
    "    val_neg = val_neg_tweeter \n",
    "    # Combine all reviews and targets for validation\n",
    "    val_x = X_val_tweeter \n",
    "    val_y = np.concatenate([y_val_tweeter ])\n",
    "\n",
    "\n",
    "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 9000\n",
      "length of val_x 1000\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'followfriday': 3,\n",
       " 'for': 4,\n",
       " 'be': 5,\n",
       " 'top': 6,\n",
       " 'engag': 7,\n",
       " 'member': 8,\n",
       " 'in': 9,\n",
       " 'my': 10,\n",
       " 'commun': 11,\n",
       " 'thi': 12,\n",
       " 'week': 13,\n",
       " ':)': 14,\n",
       " 'hey': 15,\n",
       " 'jame': 16,\n",
       " 'how': 17,\n",
       " 'odd': 18,\n",
       " ':/': 19,\n",
       " 'pleas': 20,\n",
       " 'call': 21,\n",
       " 'our': 22,\n",
       " 'contact': 23,\n",
       " 'centr': 24,\n",
       " 'on': 25,\n",
       " '02392441234': 26,\n",
       " 'and': 27,\n",
       " 'we': 28,\n",
       " 'will': 29,\n",
       " 'abl': 30,\n",
       " 'to': 31,\n",
       " 'assist': 32,\n",
       " 'you': 33,\n",
       " 'mani': 34,\n",
       " 'thank': 35,\n",
       " 'had': 36,\n",
       " 'a': 37,\n",
       " 'listen': 38,\n",
       " 'last': 39,\n",
       " 'night': 40,\n",
       " 'as': 41,\n",
       " 'bleed': 42,\n",
       " 'is': 43,\n",
       " 'an': 44,\n",
       " 'amaz': 45,\n",
       " 'track': 46,\n",
       " 'when': 47,\n",
       " 'are': 48,\n",
       " 'scotland': 49,\n",
       " 'congrat': 50,\n",
       " 'yeaaah': 51,\n",
       " 'yipppi': 52,\n",
       " 'accnt': 53,\n",
       " 'verifi': 54,\n",
       " 'rqst': 55,\n",
       " 'ha': 56,\n",
       " 'succeed': 57,\n",
       " 'got': 58,\n",
       " 'blue': 59,\n",
       " 'tick': 60,\n",
       " 'mark': 61,\n",
       " 'fb': 62,\n",
       " 'profil': 63,\n",
       " '15': 64,\n",
       " 'day': 65,\n",
       " 'one': 66,\n",
       " 'irresist': 67,\n",
       " 'flipkartfashionfriday': 68,\n",
       " \"don't\": 69,\n",
       " 'like': 70,\n",
       " 'keep': 71,\n",
       " 'love': 72,\n",
       " 'custom': 73,\n",
       " 'wait': 74,\n",
       " 'long': 75,\n",
       " 'hope': 76,\n",
       " 'enjoy': 77,\n",
       " 'happi': 78,\n",
       " 'friday': 79,\n",
       " 'lwwf': 80,\n",
       " 'second': 81,\n",
       " 'thought': 82,\n",
       " 'there': 83,\n",
       " '‚Äô': 84,\n",
       " 's': 85,\n",
       " 'just': 86,\n",
       " 'not': 87,\n",
       " 'enough': 88,\n",
       " 'time': 89,\n",
       " 'dd': 90,\n",
       " 'but': 91,\n",
       " 'new': 92,\n",
       " 'short': 93,\n",
       " 'enter': 94,\n",
       " 'system': 95,\n",
       " 'sheep': 96,\n",
       " 'must': 97,\n",
       " 'buy': 98,\n",
       " 'jgh': 99,\n",
       " 'have': 100,\n",
       " 'go': 101,\n",
       " 'bayan': 102,\n",
       " ':d': 103,\n",
       " 'bye': 104,\n",
       " 'act': 105,\n",
       " 'of': 106,\n",
       " 'mischiev': 107,\n",
       " 'am': 108,\n",
       " 'the': 109,\n",
       " 'etl': 110,\n",
       " 'layer': 111,\n",
       " 'in-hous': 112,\n",
       " 'wareh': 113,\n",
       " 'app': 114,\n",
       " 'katamari': 115,\n",
       " 'well': 116,\n",
       " '‚Ä¶': 117,\n",
       " 'name': 118,\n",
       " 'impli': 119,\n",
       " ':p': 120,\n",
       " 'influenc': 121,\n",
       " 'who': 122,\n",
       " \"wouldn't\": 123,\n",
       " 'these': 124,\n",
       " 'big': 125,\n",
       " '...': 126,\n",
       " 'juici': 127,\n",
       " 'selfi': 128,\n",
       " 'follow': 129,\n",
       " 'perfect': 130,\n",
       " 'so': 131,\n",
       " 'alreadi': 132,\n",
       " 'know': 133,\n",
       " \"what'\": 134,\n",
       " 'great': 135,\n",
       " 'opportun': 136,\n",
       " 'junior': 137,\n",
       " 'triathlet': 138,\n",
       " 'age': 139,\n",
       " '12': 140,\n",
       " '13': 141,\n",
       " 'at': 142,\n",
       " 'gatorad': 143,\n",
       " 'seri': 144,\n",
       " 'get': 145,\n",
       " 'your': 146,\n",
       " 'entri': 147,\n",
       " 'lay': 148,\n",
       " 'out': 149,\n",
       " 'greet': 150,\n",
       " 'card': 151,\n",
       " 'rang': 152,\n",
       " 'print': 153,\n",
       " 'today': 154,\n",
       " 'job': 155,\n",
       " ':-)': 156,\n",
       " \"friend'\": 157,\n",
       " 'lunch': 158,\n",
       " 'yummm': 159,\n",
       " 'nostalgia': 160,\n",
       " 'tb': 161,\n",
       " 'ku': 162,\n",
       " 'it': 163,\n",
       " 'id': 164,\n",
       " 'conflict': 165,\n",
       " 'help': 166,\n",
       " \"here'\": 167,\n",
       " 'screenshot': 168,\n",
       " 'work': 169,\n",
       " 'hi': 170,\n",
       " 'liv': 171,\n",
       " 'hello': 172,\n",
       " 'i': 173,\n",
       " 'need': 174,\n",
       " 'someth': 175,\n",
       " 'can': 176,\n",
       " 'u': 177,\n",
       " 'fm': 178,\n",
       " 'me': 179,\n",
       " 'twitter': 180,\n",
       " '‚Äî': 181,\n",
       " 'sure': 182,\n",
       " 'thing': 183,\n",
       " 'dm': 184,\n",
       " 'x': 185,\n",
       " \"i'v\": 186,\n",
       " 'heard': 187,\n",
       " 'four': 188,\n",
       " 'season': 189,\n",
       " 'pretti': 190,\n",
       " 'dope': 191,\n",
       " 'penthous': 192,\n",
       " 'obv': 193,\n",
       " 'gobigorgohom': 194,\n",
       " 'fun': 195,\n",
       " \"y'all\": 196,\n",
       " 'yeah': 197,\n",
       " 'suppos': 198,\n",
       " 'she': 199,\n",
       " 'wa': 200,\n",
       " 'lol': 201,\n",
       " 'chat': 202,\n",
       " 'bit': 203,\n",
       " 'off': 204,\n",
       " 'youth': 205,\n",
       " 'üíÖüèΩ': 206,\n",
       " 'üíã': 207,\n",
       " \"haven't\": 208,\n",
       " 'seen': 209,\n",
       " 'year': 210,\n",
       " 'rest': 211,\n",
       " 'goe': 212,\n",
       " 'by': 213,\n",
       " 'quickli': 214,\n",
       " 'bed': 215,\n",
       " 'music': 216,\n",
       " 'fix': 217,\n",
       " 'now': 218,\n",
       " 'dream': 219,\n",
       " 'spiritu': 220,\n",
       " 'ritual': 221,\n",
       " 'festiv': 222,\n",
       " 'n√©pal': 223,\n",
       " 'begin': 224,\n",
       " 'line-up': 225,\n",
       " 'left': 226,\n",
       " 'y': 227,\n",
       " 'see': 228,\n",
       " 'more': 229,\n",
       " 'sarah': 230,\n",
       " 'send': 231,\n",
       " 'us': 232,\n",
       " 'email': 233,\n",
       " 'bitsy@bitdefender.com': 234,\n",
       " \"we'll\": 235,\n",
       " 'asap': 236,\n",
       " 'kik': 237,\n",
       " 'hatessuc': 238,\n",
       " '32429': 239,\n",
       " 'kikm': 240,\n",
       " 'lgbt': 241,\n",
       " 'tinder': 242,\n",
       " 'nsfw': 243,\n",
       " 'akua': 244,\n",
       " 'cumshot': 245,\n",
       " 'come': 246,\n",
       " 'hous': 247,\n",
       " 'nsn_supplement': 248,\n",
       " 'effect': 249,\n",
       " 'press': 250,\n",
       " 'releas': 251,\n",
       " 'distribut': 252,\n",
       " 'with': 253,\n",
       " 'result': 254,\n",
       " 'link': 255,\n",
       " 'remov': 256,\n",
       " 'pressreleas': 257,\n",
       " 'newsdistribut': 258,\n",
       " 'bam': 259,\n",
       " 'bestfriend': 260,\n",
       " 'lot': 261,\n",
       " 'warsaw': 262,\n",
       " '<3': 263,\n",
       " 'x46': 264,\n",
       " 'everyon': 265,\n",
       " 'watch': 266,\n",
       " 'documentari': 267,\n",
       " 'earthl': 268,\n",
       " 'youtub': 269,\n",
       " 'support': 270,\n",
       " 'buuut': 271,\n",
       " 'oh': 272,\n",
       " 'look': 273,\n",
       " 'forward': 274,\n",
       " 'visit': 275,\n",
       " 'next': 276,\n",
       " 'letsgetmessi': 277,\n",
       " 'jo': 278,\n",
       " 'if': 279,\n",
       " 'make': 280,\n",
       " 'feel': 281,\n",
       " 'better': 282,\n",
       " 'never': 283,\n",
       " 'nor': 284,\n",
       " 'anyon': 285,\n",
       " 'kpop': 286,\n",
       " 'flesh': 287,\n",
       " 'good': 288,\n",
       " 'girl': 289,\n",
       " 'best': 290,\n",
       " 'wish': 291,\n",
       " 'reason': 292,\n",
       " 'epic': 293,\n",
       " 'soundtrack': 294,\n",
       " 'shout': 295,\n",
       " 'ad': 296,\n",
       " 'video': 297,\n",
       " 'playlist': 298,\n",
       " 'would': 299,\n",
       " 'dear': 300,\n",
       " 'jordan': 301,\n",
       " 'okay': 302,\n",
       " 'fake': 303,\n",
       " 'gameplay': 304,\n",
       " ';)': 305,\n",
       " 'haha': 306,\n",
       " 'im': 307,\n",
       " 'kid': 308,\n",
       " 'do': 309,\n",
       " 'stuff': 310,\n",
       " 'exactli': 311,\n",
       " 'product': 312,\n",
       " 'line': 313,\n",
       " 'etsi': 314,\n",
       " 'shop': 315,\n",
       " 'check': 316,\n",
       " 'vacat': 317,\n",
       " 'they': 318,\n",
       " 'recharg': 319,\n",
       " 'normal': 320,\n",
       " 'charger': 321,\n",
       " \"she'\": 322,\n",
       " 'asleep': 323,\n",
       " 'no': 324,\n",
       " 'talk': 325,\n",
       " 'sooo': 326,\n",
       " 'someon': 327,\n",
       " 'text': 328,\n",
       " 'ye': 329,\n",
       " 'bet': 330,\n",
       " \"he'll\": 331,\n",
       " 'fit': 332,\n",
       " 'after': 333,\n",
       " 'hear': 334,\n",
       " 'her': 335,\n",
       " 'speech': 336,\n",
       " 'piti': 337,\n",
       " 'green': 338,\n",
       " 'garden': 339,\n",
       " 'midnight': 340,\n",
       " 'sun': 341,\n",
       " 'beauti': 342,\n",
       " 'canal': 343,\n",
       " 'dasvidaniya': 344,\n",
       " 'till': 345,\n",
       " 'scout': 346,\n",
       " 'sg': 347,\n",
       " 'futur': 348,\n",
       " 'wlan': 349,\n",
       " 'pro': 350,\n",
       " 'confer': 351,\n",
       " 'here': 352,\n",
       " 'asia': 353,\n",
       " 'chang': 354,\n",
       " 'lollipop': 355,\n",
       " 'üç≠': 356,\n",
       " 'nez': 357,\n",
       " 'agnezmo': 358,\n",
       " 'oley': 359,\n",
       " 'mama': 360,\n",
       " 'onli': 361,\n",
       " 'whi': 362,\n",
       " 'stand': 363,\n",
       " 'stronger': 364,\n",
       " 'up': 365,\n",
       " 'god': 366,\n",
       " 'misti': 367,\n",
       " 'babi': 368,\n",
       " 'cute': 369,\n",
       " 'woohoo': 370,\n",
       " \"can't\": 371,\n",
       " 'sign': 372,\n",
       " 'yet': 373,\n",
       " 'or': 374,\n",
       " 'still': 375,\n",
       " 'think': 376,\n",
       " 'about': 377,\n",
       " 'mka': 378,\n",
       " 'liam': 379,\n",
       " 'access': 380,\n",
       " 'most': 381,\n",
       " 'welcom': 382,\n",
       " 'stat': 383,\n",
       " 'arriv': 384,\n",
       " '1': 385,\n",
       " 'unfollow': 386,\n",
       " 'via': 387,\n",
       " \"shouldn't\": 388,\n",
       " 'surpris': 389,\n",
       " 'figur': 390,\n",
       " 'happybirthdayemilybett': 391,\n",
       " 'all': 392,\n",
       " 'sweet': 393,\n",
       " 'talent': 394,\n",
       " '2': 395,\n",
       " 'plan': 396,\n",
       " 'down': 397,\n",
       " 'drain': 398,\n",
       " 'gotta': 399,\n",
       " 'timezon': 400,\n",
       " 'parent': 401,\n",
       " 'proud': 402,\n",
       " 'least': 403,\n",
       " 'mayb': 404,\n",
       " 'sometim': 405,\n",
       " 'becaus': 406,\n",
       " 'grade': 407,\n",
       " 'al': 408,\n",
       " 'grand': 409,\n",
       " 'manila_bro': 410,\n",
       " 'chosen': 411,\n",
       " 'let': 412,\n",
       " \"you'r\": 413,\n",
       " 'around': 414,\n",
       " '..': 415,\n",
       " 'side': 416,\n",
       " 'world': 417,\n",
       " 'eh': 418,\n",
       " 'too': 419,\n",
       " 'take': 420,\n",
       " 'care': 421,\n",
       " 'final': 422,\n",
       " 'fuck': 423,\n",
       " 'weekend': 424,\n",
       " 'real': 425,\n",
       " 'x45': 426,\n",
       " 'join': 427,\n",
       " 'hushedcallwithfraydo': 428,\n",
       " 'gift': 429,\n",
       " 'from': 430,\n",
       " 'yeahhh': 431,\n",
       " 'hushedpinwithsammi': 432,\n",
       " 'event': 433,\n",
       " 'might': 434,\n",
       " 'luv': 435,\n",
       " 'realli': 436,\n",
       " 'appreci': 437,\n",
       " 'share': 438,\n",
       " 'wow': 439,\n",
       " 'that': 440,\n",
       " 'tom': 441,\n",
       " 'gym': 442,\n",
       " 'monday': 443,\n",
       " 'invit': 444,\n",
       " 'scope': 445,\n",
       " 'those': 446,\n",
       " 'friend': 447,\n",
       " 'themselv': 448,\n",
       " 'nude': 449,\n",
       " 'sleep': 450,\n",
       " 'birthday': 451,\n",
       " 'want': 452,\n",
       " 't-shirt': 453,\n",
       " 'cool': 454,\n",
       " 'haw': 455,\n",
       " 'phela': 456,\n",
       " 'mom': 457,\n",
       " 'obvious': 458,\n",
       " 'him': 459,\n",
       " 'princ': 460,\n",
       " 'charm': 461,\n",
       " 'stage': 462,\n",
       " 'luck': 463,\n",
       " 'tyler': 464,\n",
       " 'hipster': 465,\n",
       " 'glass': 466,\n",
       " 'marti': 467,\n",
       " 'glad': 468,\n",
       " 'again': 469,\n",
       " 'done': 470,\n",
       " 'afternoon': 471,\n",
       " 'read': 472,\n",
       " 'kahfi': 473,\n",
       " 'befor': 474,\n",
       " 'finish': 475,\n",
       " 'ohmyg': 476,\n",
       " 'yaya': 477,\n",
       " 'dub': 478,\n",
       " 'stalk': 479,\n",
       " 'ig': 480,\n",
       " 'gondooo': 481,\n",
       " 'moo': 482,\n",
       " 'tologooo': 483,\n",
       " 'becom': 484,\n",
       " 'detail': 485,\n",
       " 'zzz': 486,\n",
       " 'xx': 487,\n",
       " 'physiotherapi': 488,\n",
       " 'hashtag': 489,\n",
       " 'üí™': 490,\n",
       " 'monica': 491,\n",
       " 'miss': 492,\n",
       " 'sound': 493,\n",
       " 'morn': 494,\n",
       " \"that'\": 495,\n",
       " 'x43': 496,\n",
       " 'definit': 497,\n",
       " 'tri': 498,\n",
       " 'tonight': 499,\n",
       " 'then': 500,\n",
       " 'took': 501,\n",
       " 'advic': 502,\n",
       " 'treviso': 503,\n",
       " 'concert': 504,\n",
       " 'citi': 505,\n",
       " 'countri': 506,\n",
       " \"i'll\": 507,\n",
       " 'start': 508,\n",
       " 'fine': 509,\n",
       " 'gorgeou': 510,\n",
       " 'xo': 511,\n",
       " 'oven': 512,\n",
       " 'roast': 513,\n",
       " 'garlic': 514,\n",
       " 'oliv': 515,\n",
       " 'oil': 516,\n",
       " 'dri': 517,\n",
       " 'tomato': 518,\n",
       " 'some': 519,\n",
       " 'basil': 520,\n",
       " 'centuri': 521,\n",
       " 'tuna': 522,\n",
       " 'right': 523,\n",
       " 'back': 524,\n",
       " 'atchya': 525,\n",
       " \"doesn't\": 526,\n",
       " 'even': 527,\n",
       " 'almost': 528,\n",
       " 'chanc': 529,\n",
       " 'cheer': 530,\n",
       " 'po': 531,\n",
       " 'ice': 532,\n",
       " 'cream': 533,\n",
       " 'agre': 534,\n",
       " '100': 535,\n",
       " 'heheheh': 536,\n",
       " 'point': 537,\n",
       " 'stay': 538,\n",
       " 'home': 539,\n",
       " 'soon': 540,\n",
       " 'promis': 541,\n",
       " 'web': 542,\n",
       " 'whatsapp': 543,\n",
       " 'volta': 544,\n",
       " 'funcionar': 545,\n",
       " 'com': 546,\n",
       " 'iphon': 547,\n",
       " 'jailbroken': 548,\n",
       " 'later': 549,\n",
       " '34': 550,\n",
       " 'min': 551,\n",
       " 'leia': 552,\n",
       " 'appear': 553,\n",
       " 'hologram': 554,\n",
       " 'r2d2': 555,\n",
       " 'w': 556,\n",
       " 'messag': 557,\n",
       " 'obi': 558,\n",
       " 'wan': 559,\n",
       " 'he': 560,\n",
       " 'sit': 561,\n",
       " 'luke': 562,\n",
       " 'inter': 563,\n",
       " '3': 564,\n",
       " 'ucl': 565,\n",
       " 'arsen': 566,\n",
       " 'small': 567,\n",
       " 'team': 568,\n",
       " 'pass': 569,\n",
       " 'üöÇ': 570,\n",
       " 'dewsburi': 571,\n",
       " 'railway': 572,\n",
       " 'station': 573,\n",
       " 'dew': 574,\n",
       " 'west': 575,\n",
       " 'yorkshir': 576,\n",
       " '430': 577,\n",
       " 'smh': 578,\n",
       " \"it'\": 579,\n",
       " '9:25': 580,\n",
       " 'live': 581,\n",
       " 'strang': 582,\n",
       " 'imagin': 583,\n",
       " 'what': 584,\n",
       " 'megan': 585,\n",
       " 'masaantoday': 586,\n",
       " 'a4': 587,\n",
       " 'shweta': 588,\n",
       " 'tripathi': 589,\n",
       " '5': 590,\n",
       " 'over': 591,\n",
       " '20': 592,\n",
       " 'kurta': 593,\n",
       " 'half': 594,\n",
       " 'number': 595,\n",
       " 'wsalelov': 596,\n",
       " 'ah': 597,\n",
       " 'larri': 598,\n",
       " 'anyway': 599,\n",
       " 'kinda': 600,\n",
       " 'goood': 601,\n",
       " 'life': 602,\n",
       " 'enn': 603,\n",
       " 'could': 604,\n",
       " 'warmup': 605,\n",
       " '15th': 606,\n",
       " 'bath': 607,\n",
       " 'dum': 608,\n",
       " 'andar': 609,\n",
       " 'ram': 610,\n",
       " 'sampath': 611,\n",
       " 'sona': 612,\n",
       " 'mohapatra': 613,\n",
       " 'samantha': 614,\n",
       " 'edward': 615,\n",
       " 'mein': 616,\n",
       " 'tulan': 617,\n",
       " 'razi': 618,\n",
       " 'wah': 619,\n",
       " 'josh': 620,\n",
       " 'alway': 621,\n",
       " 'smile': 622,\n",
       " 'pictur': 623,\n",
       " '16.20': 624,\n",
       " 'giveitup': 625,\n",
       " 'given': 626,\n",
       " 'ga': 627,\n",
       " 'subsidi': 628,\n",
       " 'initi': 629,\n",
       " 'propos': 630,\n",
       " 'delight': 631,\n",
       " 'yesterday': 632,\n",
       " 'x42': 633,\n",
       " 'lmaoo': 634,\n",
       " 'song': 635,\n",
       " 'ever': 636,\n",
       " 'shall': 637,\n",
       " 'own': 638,\n",
       " 'littl': 639,\n",
       " 'throwback': 640,\n",
       " 'outli': 641,\n",
       " 'island': 642,\n",
       " 'such': 643,\n",
       " 'cheung': 644,\n",
       " 'chau': 645,\n",
       " 'mui': 646,\n",
       " 'wo': 647,\n",
       " 'total': 648,\n",
       " 'differ': 649,\n",
       " 'kfckitchentour': 650,\n",
       " 'kitchen': 651,\n",
       " 'clean': 652,\n",
       " \"i'm\": 653,\n",
       " 'cusp': 654,\n",
       " 'test': 655,\n",
       " 'water': 656,\n",
       " 'reward': 657,\n",
       " 'arummzz': 658,\n",
       " \"let'\": 659,\n",
       " 'drive': 660,\n",
       " 'travel': 661,\n",
       " 'yogyakarta': 662,\n",
       " 'jeep': 663,\n",
       " 'indonesia': 664,\n",
       " 'instamood': 665,\n",
       " 'wanna': 666,\n",
       " 'skype': 667,\n",
       " 'may': 668,\n",
       " 'nice': 669,\n",
       " 'friendli': 670,\n",
       " 'them': 671,\n",
       " 'pretend': 672,\n",
       " 'where': 673,\n",
       " 'film': 674,\n",
       " 'congratul': 675,\n",
       " 'winner': 676,\n",
       " 'cheesydelight': 677,\n",
       " 'contest': 678,\n",
       " 'address': 679,\n",
       " 'guy': 680,\n",
       " 'market': 681,\n",
       " '24/7': 682,\n",
       " '14': 683,\n",
       " 'hour': 684,\n",
       " 'leav': 685,\n",
       " 'without': 686,\n",
       " 'delay': 687,\n",
       " 'actual': 688,\n",
       " 'veri': 689,\n",
       " 'easi': 690,\n",
       " 'guess': 691,\n",
       " 'train': 692,\n",
       " 'wd': 693,\n",
       " 'shift': 694,\n",
       " 'engin': 695,\n",
       " 'etc': 696,\n",
       " 'sunburn': 697,\n",
       " 'peel': 698,\n",
       " 'blog': 699,\n",
       " 'huge': 700,\n",
       " 'warm': 701,\n",
       " '‚òÜ': 702,\n",
       " 'complet': 703,\n",
       " 'triangl': 704,\n",
       " 'northern': 705,\n",
       " 'ireland': 706,\n",
       " 'sight': 707,\n",
       " 'smthng': 708,\n",
       " 'fr': 709,\n",
       " 'hug': 710,\n",
       " 'xoxo': 711,\n",
       " 'uu': 712,\n",
       " 'jaann': 713,\n",
       " 'topnewfollow': 714,\n",
       " 'connect': 715,\n",
       " 'wonder': 716,\n",
       " 'made': 717,\n",
       " 'fluffi': 718,\n",
       " 'insid': 719,\n",
       " 'pirouett': 720,\n",
       " 'moos': 721,\n",
       " 'trip': 722,\n",
       " 'philli': 723,\n",
       " 'decemb': 724,\n",
       " \"i'd\": 725,\n",
       " 'dude': 726,\n",
       " 'x41': 727,\n",
       " 'question': 728,\n",
       " 'flaw': 729,\n",
       " 'pain': 730,\n",
       " 'negat': 731,\n",
       " 'strength': 732,\n",
       " 'went': 733,\n",
       " 'solo': 734,\n",
       " 'move': 735,\n",
       " \"weren't\": 736,\n",
       " 'fav': 737,\n",
       " 'nirvana': 738,\n",
       " 'smell': 739,\n",
       " 'teen': 740,\n",
       " 'spirit': 741,\n",
       " 'rip': 742,\n",
       " 'ami': 743,\n",
       " 'winehous': 744,\n",
       " 'did': 745,\n",
       " 'coupl': 746,\n",
       " 'tomhiddleston': 747,\n",
       " 'elizabetholsen': 748,\n",
       " 'yaytheylookgreat': 749,\n",
       " 'goodnight': 750,\n",
       " 'vid': 751,\n",
       " 'wake': 752,\n",
       " 'gonna': 753,\n",
       " 'shoot': 754,\n",
       " 'itti': 755,\n",
       " 'bitti': 756,\n",
       " 'teeni': 757,\n",
       " 'bikini': 758,\n",
       " 'much': 759,\n",
       " '4th': 760,\n",
       " 'togeth': 761,\n",
       " 'end': 762,\n",
       " 'xfile': 763,\n",
       " 'content': 764,\n",
       " 'rain': 765,\n",
       " 'fabul': 766,\n",
       " 'fantast': 767,\n",
       " '‚ô°': 768,\n",
       " 'jb': 769,\n",
       " 'forev': 770,\n",
       " 'belieb': 771,\n",
       " 'nighti': 772,\n",
       " 'bug': 773,\n",
       " 'bite': 774,\n",
       " 'bracelet': 775,\n",
       " 'idea': 776,\n",
       " 'foundri': 777,\n",
       " 'game': 778,\n",
       " 'sens': 779,\n",
       " \"didn't\": 780,\n",
       " 'pic': 781,\n",
       " 'ef': 782,\n",
       " 'phone': 783,\n",
       " 'woot': 784,\n",
       " 'derek': 785,\n",
       " 'use': 786,\n",
       " 'parkshar': 787,\n",
       " 'gloucestershir': 788,\n",
       " 'aaaahhh': 789,\n",
       " 'man': 790,\n",
       " 'traffic': 791,\n",
       " 'stress': 792,\n",
       " 'reliev': 793,\n",
       " \"how'r\": 794,\n",
       " 'arbeloa': 795,\n",
       " 'turn': 796,\n",
       " '17': 797,\n",
       " 'omg': 798,\n",
       " 'say': 799,\n",
       " 'europ': 800,\n",
       " 'rise': 801,\n",
       " 'find': 802,\n",
       " 'hard': 803,\n",
       " 'believ': 804,\n",
       " 'uncount': 805,\n",
       " 'coz': 806,\n",
       " 'unlimit': 807,\n",
       " 'cours': 808,\n",
       " 'teamposit': 809,\n",
       " 'aldub': 810,\n",
       " '‚òï': 811,\n",
       " 'rita': 812,\n",
       " 'further': 813,\n",
       " 'info': 814,\n",
       " \"we'd\": 815,\n",
       " 'way': 816,\n",
       " 'boy': 817,\n",
       " 'x40': 818,\n",
       " 'true': 819,\n",
       " 'sethi': 820,\n",
       " 'high': 821,\n",
       " 'exe': 822,\n",
       " 'skeem': 823,\n",
       " 'saam': 824,\n",
       " 'peopl': 825,\n",
       " 'polit': 826,\n",
       " 'izzat': 827,\n",
       " 'wese': 828,\n",
       " 'doe': 829,\n",
       " 'trust': 830,\n",
       " 'khawateen': 831,\n",
       " 'k': 832,\n",
       " 'sath': 833,\n",
       " 'mana': 834,\n",
       " 'kar': 835,\n",
       " 'deya': 836,\n",
       " 'sort': 837,\n",
       " 'smart': 838,\n",
       " 'hair': 839,\n",
       " 'tbh': 840,\n",
       " 'jacob': 841,\n",
       " 'm': 842,\n",
       " 'g': 843,\n",
       " 'upgrad': 844,\n",
       " 'tee': 845,\n",
       " 'famili': 846,\n",
       " 'person': 847,\n",
       " 'two': 848,\n",
       " 'convers': 849,\n",
       " 'should': 850,\n",
       " 'onlin': 851,\n",
       " 'mclaren': 852,\n",
       " 'fridayfeel': 853,\n",
       " 'tgif': 854,\n",
       " 'squar': 855,\n",
       " 'enix': 856,\n",
       " 'bissmillah': 857,\n",
       " 'ya': 858,\n",
       " 'allah': 859,\n",
       " \"we'r\": 860,\n",
       " 'socent': 861,\n",
       " 'startup': 862,\n",
       " 'drop': 863,\n",
       " 'arnd': 864,\n",
       " 'town': 865,\n",
       " 'basic': 866,\n",
       " 'piss': 867,\n",
       " 'cup': 868,\n",
       " 'also': 869,\n",
       " 'terribl': 870,\n",
       " 'complic': 871,\n",
       " 'discuss': 872,\n",
       " 'snapchat': 873,\n",
       " 'lynettelow': 874,\n",
       " 'kikmenow': 875,\n",
       " 'snapm': 876,\n",
       " 'hot': 877,\n",
       " 'amazon': 878,\n",
       " 'kikmeguy': 879,\n",
       " 'defin': 880,\n",
       " 'grow': 881,\n",
       " 'sport': 882,\n",
       " 'rt': 883,\n",
       " 'rakyat': 884,\n",
       " 'write': 885,\n",
       " 'sinc': 886,\n",
       " 'mention': 887,\n",
       " 'fli': 888,\n",
       " 'fish': 889,\n",
       " 'other': 890,\n",
       " 'promot': 891,\n",
       " 'post': 892,\n",
       " 'cyber': 893,\n",
       " 'ourdaughtersourprid': 894,\n",
       " 'mypapamyprid': 895,\n",
       " 'papa': 896,\n",
       " 'coach': 897,\n",
       " 'posit': 898,\n",
       " 'kha': 899,\n",
       " 'atleast': 900,\n",
       " 'x39': 901,\n",
       " 'mango': 902,\n",
       " \"lassi'\": 903,\n",
       " \"monty'\": 904,\n",
       " 'marvel': 905,\n",
       " 'though': 906,\n",
       " 'suspect': 907,\n",
       " 'meant': 908,\n",
       " '24': 909,\n",
       " 'hr': 910,\n",
       " 'touch': 911,\n",
       " 'kepler': 912,\n",
       " '452b': 913,\n",
       " 'chalna': 914,\n",
       " 'hai': 915,\n",
       " 'thankyou': 916,\n",
       " 'hazel': 917,\n",
       " 'food': 918,\n",
       " 'brooklyn': 919,\n",
       " 'pta': 920,\n",
       " 'awak': 921,\n",
       " 'okayi': 922,\n",
       " 'awww': 923,\n",
       " 'doc': 924,\n",
       " 'splendid': 925,\n",
       " 'spam': 926,\n",
       " 'folder': 927,\n",
       " 'won': 928,\n",
       " 'amount': 929,\n",
       " 'nigeria': 930,\n",
       " 'claim': 931,\n",
       " 'rted': 932,\n",
       " 'leg': 933,\n",
       " 'hurt': 934,\n",
       " 'bad': 935,\n",
       " 'mine': 936,\n",
       " 'saturday': 937,\n",
       " 'thaaank': 938,\n",
       " 'puhon': 939,\n",
       " 'happinesss': 940,\n",
       " 'tnc': 941,\n",
       " 'prior': 942,\n",
       " 'notif': 943,\n",
       " 'fat': 944,\n",
       " 'co': 945,\n",
       " 'probabl': 946,\n",
       " 'ate': 947,\n",
       " 'yuna': 948,\n",
       " 'tamesid': 949,\n",
       " '¬¥': 950,\n",
       " 're': 951,\n",
       " 'googl': 952,\n",
       " 'account': 953,\n",
       " 'scouser': 954,\n",
       " 'everyth': 955,\n",
       " 'zoe': 956,\n",
       " 'mate': 957,\n",
       " 'liter': 958,\n",
       " \"they'r\": 959,\n",
       " 'samee': 960,\n",
       " 'edgar': 961,\n",
       " 'updat': 962,\n",
       " 'log': 963,\n",
       " 'bring': 964,\n",
       " 'abe': 965,\n",
       " 'same': 966,\n",
       " 'meet': 967,\n",
       " 'x38': 968,\n",
       " 'sigh': 969,\n",
       " 'dreamili': 970,\n",
       " 'pout': 971,\n",
       " 'eye': 972,\n",
       " 'quacketyquack': 973,\n",
       " 'funni': 974,\n",
       " 'happen': 975,\n",
       " 'phil': 976,\n",
       " 'em': 977,\n",
       " 'del': 978,\n",
       " 'rodder': 979,\n",
       " 'els': 980,\n",
       " 'play': 981,\n",
       " 'newest': 982,\n",
       " 'gamejam': 983,\n",
       " 'irish': 984,\n",
       " 'literatur': 985,\n",
       " 'inaccess': 986,\n",
       " \"kareena'\": 987,\n",
       " 'fan': 988,\n",
       " 'brain': 989,\n",
       " 'dot': 990,\n",
       " 'braindot': 991,\n",
       " 'fair': 992,\n",
       " 'rush': 993,\n",
       " 'either': 994,\n",
       " 'brandi': 995,\n",
       " '18': 996,\n",
       " 'dure': 997,\n",
       " 'carniv': 998,\n",
       " 'men': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "# Unit Test Note - There is no test set here only train/val\n",
    "def get_vocab(train_x, min_occurrence=2):\n",
    "\n",
    "    # Include special tokens \n",
    "    # started with pad, end of line and unk tokens\n",
    "    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "    # Count occurrences of each word in the entire dataset\n",
    "    word_counts = {}\n",
    "    for tweet in train_x:\n",
    "        processed_tweet = process_tweet(tweet)\n",
    "        for word in processed_tweet:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Add words to the vocabulary if their occurrence is greater than min_occurrence\n",
    "    for word, count in word_counts.items():\n",
    "        if count > min_occurrence and word not in Vocab:\n",
    "            Vocab[word] = len(Vocab)\n",
    "\n",
    "    return Vocab\n",
    "\n",
    "# Set the minimum occurrence for a word to be included in the vocabulary\n",
    "min_occurrence = 0\n",
    "\n",
    "Vocab = get_vocab(train_x, min_occurrence)\n",
    "\n",
    "print(\"Total words in vocab are\", len(Vocab))\n",
    "display(Vocab)\n",
    "\n",
    "with open('Vocabulory.json', 'w') as json_file:\n",
    "    json.dump(Vocab, json_file)\n",
    "\n",
    "Vocab = None\n",
    "with open('Vocabulory.json', 'r') as json_file:\n",
    "    Vocab = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_vocab = len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANDIDATE FOR TABLE TEST - If a student forgets to check for unk, there might be errors or just wrong values in the list.\n",
    "# We can add those errors to check in autograder through tabled test or here student facing user test.\n",
    "\n",
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) \n",
    "# GRADED FUNCTION: tweet_to_tensor\n",
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "    \n",
    "    '''     \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "    \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = [] \n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "    \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.        \n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "          \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID)\n",
    "    # tensor_l.append(1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " @delafro_ @unbearvble yeah the left one supposedly hurts more bc it's right over your heart apparently. It'll be fine! Good luck :)\n",
      "\n",
      "Tensor of tweet:\n",
      " [197, 109, 226, 66, 9625, 934, 229, 1912, 579, 523, 591, 146, 1143, 1221, 4671, 5, 509, 288, 463, 14]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED: Data generator\n",
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of positive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be even\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - A numpy array specifying the importance of each example\n",
    "        \n",
    "    '''     \n",
    "\n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples    \n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from 0 and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "\n",
    "        # Using the same batch list, start from 0 and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset,\n",
    "            if neg_index >= len_data_neg :\n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True \n",
    "                    break \n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "                    \n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet,vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index = neg_index+1\n",
    "\n",
    "        ### END CODE HERE ###        \n",
    "\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "\n",
    "\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = n_pad*[0]\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = n_to_take*[1]\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of zeros)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = n_to_take*[0]\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.\n",
    "        example_weights = np.ones_like(targets).astype(int)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[  35   72 1030  523  524  142   33  100   37   72   65   14    0    0\n",
      "     0    0    0    0]\n",
      " [  35   33  468   33   77  156    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [ 173   69  666  101  539   86  964  109 3330  918  591  352   27  507\n",
      "  4160  538  352 3887]\n",
      " [ 131  131  131  759 3887 1852  279   37 2370 6075 6665  163  474  579\n",
      "    89 6564    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "\n",
    "def train_generator(batch_size, train_pos\n",
    "                    , train_neg, vocab_dict, loop=True\n",
    "                    , shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, val_pos\n",
    "                    , val_neg, vocab_dict, loop=True\n",
    "                    , shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, val_pos\n",
    "                    , val_neg, vocab_dict, loop=False\n",
    "                    , shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, train_pos, train_neg, Vocab, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([[ 306, 1280,   37, 1030,    4,  183,   25,   37,   79,   43,   37,\n",
       "          669,  816,   31,  508,  109,   65,   14,  487],\n",
       "        [1889,  669,  669,  156,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [  10,  817, 1514,  981, 3887,   27,  173, 1122,  266,  163, 3887,\n",
       "          742,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [6781, 6023,  368, 3887, 2043, 2043, 2043,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32),\n",
       " Array([1, 1, 0, 0], dtype=int32),\n",
       " Array([1, 1, 1, 1], dtype=int32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for inputs , target , example_weights in train_generator(4, train_pos, train_neg, Vocab, shuffle=True):\n",
    "#     print(inputs)\n",
    "import itertools\n",
    "\n",
    "infinite_data_generator = itertools.cycle(\n",
    "    train_generator(4,train_pos, train_neg, Vocab, shuffle=True))\n",
    "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
    "print(len(ten_lines))\n",
    "ten_lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (8, 23)\n",
      "input tensor: [ 3  4  5  6  7  8  9 10 11 12 13 14  0  0  0  0  0  0  0  0  0  0  0]; target 1; example weights 1\n",
      "input tensor: [15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  5 30 31 32 33 14 34 35]; target 1; example weights 1\n",
      "input tensor: [28 36 37 38 39 40 14 41 33 42 43 44 45 46 47 48 33  9 49  0  0  0  0]; target 1; example weights 1\n",
      "input tensor: [50 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]; target 1; example weights 1\n",
      "input tensor: [6307    4 3023 3887    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 955    9  109  308 3777  106 6308   43  131  369 4586  653 1340 2888\n",
      "    9  395 1312 3887    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 440 1143 6309 1069  109 1545 2549 3887    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [1924  173 1121 1521   21  459 6310 3887 3887 2220  179  419    0    0\n",
      "    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generato\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 8, train_pos=train_pos, train_neg=train_neg, vocab_dict=Vocab)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: SentimentAnalysisModel\n",
    "def SentimentAnalysisModel(vocab_size=118675, d_model=256, n_layers=2, mode='train'):\n",
    "    \"\"\"Returns a sentiment analysis model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
    "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A sentiment analysis model as a layer that maps from a tensor of tokens to a single sentiment prediction.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    model = tl.Serial( \n",
    "        tl.Embedding(vocab_size, d_model),  # Stack the embedding layer\n",
    "        [tl.GRU(n_units=d_model,mode=mode) for _ in range(n_layers)],  # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "        tl.Select([0]),\n",
    "        tl.Mean(axis = 1),\n",
    "        tl.Dense(n_units=2),  # Adjust Dense layer for a single output unit\n",
    "        tl.LogSoftmax(),  \n",
    "    ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Embedding_9900_256\n",
      "  GRU_256\n",
      "  GRU_256\n",
      "  Mean\n",
      "  Dense_2\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# testing your model\n",
    "model = SentimentAnalysisModel(vocab_size=length_vocab)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used lines from the dataset: 9000\n",
      "Batch size (a power of 2): 16\n",
      "Number of steps to cover one epoch: 562\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Number of used lines from the dataset:', 2*len(train_pos))\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(2*len(train_pos)/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 31169282\n",
      "Step      1: Ran 1 train steps in 8.29 secs\n",
      "Step      1: train CrossEntropyLoss |  0.69315016\n",
      "Step      1: eval  CrossEntropyLoss |  0.69314921\n",
      "Step      1: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      2: Ran 1 train steps in 7.55 secs\n",
      "Step      2: train CrossEntropyLoss |  0.69349945\n",
      "Step      2: eval  CrossEntropyLoss |  0.69310892\n",
      "Step      2: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      3: Ran 1 train steps in 7.33 secs\n",
      "Step      3: train CrossEntropyLoss |  0.69243050\n",
      "Step      3: eval  CrossEntropyLoss |  0.69183654\n",
      "Step      3: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      4: Ran 1 train steps in 7.21 secs\n",
      "Step      4: train CrossEntropyLoss |  0.69615436\n",
      "Step      4: eval  CrossEntropyLoss |  0.69135463\n",
      "Step      4: eval          Accuracy |  0.68750000\n",
      "\n",
      "Step      5: Ran 1 train steps in 7.25 secs\n",
      "Step      5: train CrossEntropyLoss |  0.69248998\n",
      "Step      5: eval  CrossEntropyLoss |  0.69192523\n",
      "Step      5: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      6: Ran 1 train steps in 7.18 secs\n",
      "Step      6: train CrossEntropyLoss |  0.69249856\n",
      "Step      6: eval  CrossEntropyLoss |  0.69252872\n",
      "Step      6: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      7: Ran 1 train steps in 7.78 secs\n",
      "Step      7: train CrossEntropyLoss |  0.69137120\n",
      "Step      7: eval  CrossEntropyLoss |  0.69217402\n",
      "Step      7: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      8: Ran 1 train steps in 5.98 secs\n",
      "Step      8: train CrossEntropyLoss |  0.69150501\n",
      "Step      8: eval  CrossEntropyLoss |  0.69160473\n",
      "Step      8: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step      9: Ran 1 train steps in 7.19 secs\n",
      "Step      9: train CrossEntropyLoss |  0.69047350\n",
      "Step      9: eval  CrossEntropyLoss |  0.69892418\n",
      "Step      9: eval          Accuracy |  0.37500000\n",
      "\n",
      "Step     10: Ran 1 train steps in 7.08 secs\n",
      "Step     10: train CrossEntropyLoss |  0.68567038\n",
      "Step     10: eval  CrossEntropyLoss |  0.69237810\n",
      "Step     10: eval          Accuracy |  0.56250000\n",
      "\n",
      "Step     11: Ran 1 train steps in 7.17 secs\n",
      "Step     11: train CrossEntropyLoss |  0.67637402\n",
      "Step     11: eval  CrossEntropyLoss |  0.71059567\n",
      "Step     11: eval          Accuracy |  0.31250000\n",
      "\n",
      "Step     12: Ran 1 train steps in 7.35 secs\n",
      "Step     12: train CrossEntropyLoss |  0.66578799\n",
      "Step     12: eval  CrossEntropyLoss |  0.68780631\n",
      "Step     12: eval          Accuracy |  0.56250000\n",
      "\n",
      "Step     13: Ran 1 train steps in 5.90 secs\n",
      "Step     13: train CrossEntropyLoss |  0.63187993\n",
      "Step     13: eval  CrossEntropyLoss |  0.64940673\n",
      "Step     13: eval          Accuracy |  0.68750000\n",
      "\n",
      "Step     14: Ran 1 train steps in 7.11 secs\n",
      "Step     14: train CrossEntropyLoss |  0.61146075\n",
      "Step     14: eval  CrossEntropyLoss |  0.67415076\n",
      "Step     14: eval          Accuracy |  0.56250000\n",
      "\n",
      "Step     15: Ran 1 train steps in 5.72 secs\n",
      "Step     15: train CrossEntropyLoss |  0.78913909\n",
      "Step     15: eval  CrossEntropyLoss |  0.61959016\n",
      "Step     15: eval          Accuracy |  0.62500000\n",
      "\n",
      "Step     16: Ran 1 train steps in 5.72 secs\n",
      "Step     16: train CrossEntropyLoss |  0.82909846\n",
      "Step     16: eval  CrossEntropyLoss |  0.75537968\n",
      "Step     16: eval          Accuracy |  0.25000000\n",
      "\n",
      "Step     17: Ran 1 train steps in 5.71 secs\n",
      "Step     17: train CrossEntropyLoss |  0.67625195\n",
      "Step     17: eval  CrossEntropyLoss |  0.68618119\n",
      "Step     17: eval          Accuracy |  0.62500000\n",
      "\n",
      "Step     18: Ran 1 train steps in 5.84 secs\n",
      "Step     18: train CrossEntropyLoss |  0.65330565\n",
      "Step     18: eval  CrossEntropyLoss |  0.67772567\n",
      "Step     18: eval          Accuracy |  0.56250000\n",
      "\n",
      "Step     19: Ran 1 train steps in 5.69 secs\n",
      "Step     19: train CrossEntropyLoss |  0.67734748\n",
      "Step     19: eval  CrossEntropyLoss |  0.68362188\n",
      "Step     19: eval          Accuracy |  0.50000000\n",
      "\n",
      "Step     20: Ran 1 train steps in 5.79 secs\n",
      "Step     20: train CrossEntropyLoss |  0.68173516\n",
      "Step     20: eval  CrossEntropyLoss |  0.64284205\n",
      "Step     20: eval          Accuracy |  0.50000000\n"
     ]
    }
   ],
   "source": [
    "# PLEASE, DO NOT MODIFY OR DELETE THIS CELL\n",
    "import tensorflow as tf\n",
    "from trax.supervised import training\n",
    "# from trax import\n",
    "\n",
    "def train_model(classifier,train_pos, train_neg, val_pos, val_neg, vocab_dict, loop, batch_size , n_steps, output_dir):\n",
    "    \n",
    "    rnd.seed(271)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator(batch_size, train_pos\n",
    "                    , train_neg, vocab_dict, loop\n",
    "                    , shuffle = True),\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.001),\n",
    "        n_steps_per_checkpoint=1,\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator(batch_size, val_pos\n",
    "                    , val_neg, vocab_dict, loop\n",
    "                    , shuffle = True),        \n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    "    )\n",
    "\n",
    "    \n",
    "# return train_task, eval_task\n",
    "# rnd.seed(31) #¬†Do NOT modify this random seed. This makes the notebook easier to replicate\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###   \n",
    "    # output_dir = os.path.expanduser('~/output_dir/')\n",
    "    !rm -rf {output_dir}       \n",
    "\n",
    "    training_loop = training.Loop( \n",
    "                                classifier, # The learning model\n",
    "                                train_task, # The training task\n",
    "                                eval_tasks=[eval_task], # The evaluation task\n",
    "                                output_dir=output_dir, # The output directory\n",
    "                                random_seed=31 # Do not modify this random seed in order to ensure reproducibility and for grading purposes.\n",
    "    ) \n",
    "\n",
    "    \n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "    ### END CODE HERE ###\n",
    "    # print(training_loop.model.weights)\n",
    "\n",
    "    # Load the model from SavedModel.\n",
    "    # loaded_model = tf.keras.models.load_model(model_file)\n",
    "        \n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop\n",
    "\n",
    "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
    "output_dir = './new_model/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_dir)\n",
    "except OSError as e:\n",
    "    pass\n",
    "\n",
    "training_loop = train_model(SentimentAnalysisModel(), train_pos, train_neg, val_pos, val_neg, Vocab, True ,batch_size,20,output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ 0.01849687,  0.01459226,  0.06136378, ..., -0.05681958,\n",
       "           0.07262021, -0.02327949],\n",
       "         [ 0.0585352 ,  0.00385864, -0.00041259, ...,  0.07603992,\n",
       "           0.06545908, -0.076805  ],\n",
       "         [-0.01325028,  0.06936208, -0.02059306, ...,  0.0189916 ,\n",
       "          -0.04695515, -0.09861901],\n",
       "         ...,\n",
       "         [ 0.0569117 ,  0.03746707, -0.04773592, ..., -0.06541207,\n",
       "          -0.07834762,  0.03926533],\n",
       "         [-0.08560685,  0.04697158, -0.03526554, ..., -0.00298655,\n",
       "           0.02328494,  0.07494982],\n",
       "         [ 0.05376793,  0.02353536, -0.03720468, ...,  0.09931763,\n",
       "           0.06046603, -0.01164777]], dtype=float32),\n",
       "  (((), ((), ())),\n",
       "   ((array([[ 0.00282491,  0.01119213,  0.01060081, ...,  0.00373673,\n",
       "              0.00741243,  0.00113649],\n",
       "            [-0.00753159, -0.00045818, -0.00037899, ...,  0.00931353,\n",
       "              0.00653336, -0.00777436],\n",
       "            [ 0.00036602,  0.00560274, -0.00605689, ..., -0.0032316 ,\n",
       "              0.00337196,  0.00347064],\n",
       "            ...,\n",
       "            [ 0.00871214, -0.00362515,  0.00130386, ..., -0.00165236,\n",
       "              0.00161145,  0.00572171],\n",
       "            [-0.00084592, -0.00255537, -0.00483451, ..., -0.00210677,\n",
       "              0.0086417 , -0.00981703],\n",
       "            [-0.00801268,  0.00723352,  0.00026492, ..., -0.00290514,\n",
       "              0.00343256, -0.00557816]], dtype=float32),\n",
       "     array([ 8.1391382e-04,  1.1936694e-03,  1.4663092e-03,  1.2632910e-03,\n",
       "             1.6012473e-03,  1.5631113e-03,  1.3921995e-03,  1.1716021e-03,\n",
       "             1.3278404e-03,  1.6732627e-03,  1.5829860e-03,  1.4939911e-03,\n",
       "             1.0086764e-03,  1.0901648e-03,  2.8032070e-04,  1.1281747e-03,\n",
       "             1.3443308e-03,  1.6339954e-03,  1.5925504e-03,  1.3740115e-03,\n",
       "             1.2225735e-03,  1.5763101e-03,  1.5143579e-03,  2.3590709e-04,\n",
       "             1.4938036e-03,  1.5330378e-03,  5.4825936e-04,  1.1908517e-03,\n",
       "             1.3710522e-03,  1.9316630e-04,  1.6951668e-03,  1.2084347e-03,\n",
       "             1.1235019e-03,  1.1618263e-03,  1.4120841e-03,  1.3398164e-03,\n",
       "             1.5732177e-03,  1.4061874e-03,  1.4320415e-03,  9.7247929e-04,\n",
       "             1.3949586e-03,  1.6533035e-03,  1.5249526e-03,  1.3140313e-03,\n",
       "             1.1811008e-03,  1.4902368e-03,  1.2809201e-03,  1.4236576e-03,\n",
       "             1.4859537e-03,  1.5446756e-03,  1.1896524e-03,  9.0760773e-04,\n",
       "             1.4001010e-03,  9.4437762e-04,  1.4907480e-03,  1.5733917e-03,\n",
       "             8.2132249e-04,  8.8775769e-04,  9.2682679e-04,  1.5303920e-03,\n",
       "             1.2882174e-03,  1.4407452e-03,  9.0096978e-04,  3.2417462e-04,\n",
       "             1.5009886e-03,  1.6349041e-03,  1.4014139e-03,  1.5398363e-03,\n",
       "             1.6036720e-03,  1.4460223e-03,  1.4827616e-03,  1.6334470e-03,\n",
       "             1.5796206e-03,  1.1837466e-03,  1.5264974e-03,  2.9085437e-04,\n",
       "             8.4584462e-04,  1.1840343e-03,  5.4383784e-04,  1.5379746e-03,\n",
       "             1.6254543e-03,  1.6924237e-03,  1.2355379e-03,  6.5841054e-04,\n",
       "             1.2536884e-03,  1.3273745e-03,  1.6892470e-03,  1.2937459e-03,\n",
       "             1.3393206e-03,  1.4531733e-03,  1.4765514e-03,  1.2374888e-03,\n",
       "             1.3328881e-03,  1.0774320e-03,  1.4270770e-03,  1.0785642e-03,\n",
       "             1.2212807e-03,  1.6132001e-03,  1.3730872e-03,  1.3295853e-03,\n",
       "             1.3221180e-03,  4.0924168e-04,  1.5940282e-03,  1.4786937e-03,\n",
       "             1.4160720e-03,  1.5004027e-03,  1.5374154e-03,  1.4519702e-03,\n",
       "             1.0794719e-03,  1.4747032e-03,  9.2623284e-04,  1.1357259e-03,\n",
       "             1.5556362e-03,  1.1658495e-03,  1.2001300e-03,  1.5977982e-03,\n",
       "             1.0266292e-03,  1.5466104e-03,  1.5481395e-03,  1.5683911e-03,\n",
       "             1.2592098e-03,  1.6833149e-03,  1.6768397e-03,  1.5178999e-03,\n",
       "             1.3243514e-03,  1.5858096e-03,  1.4204833e-03,  9.9496369e-04,\n",
       "             1.4414422e-03,  1.3771993e-03,  1.4848321e-03,  1.4384005e-03,\n",
       "             1.7065729e-03,  5.3119886e-04,  1.4168592e-03,  1.2467245e-03,\n",
       "             1.3080009e-03,  1.2222253e-03,  1.4797930e-03,  1.2476149e-03,\n",
       "             1.5572272e-03,  1.4073525e-03,  1.1782866e-03,  1.2163050e-03,\n",
       "             1.6993372e-03,  1.1161497e-03,  1.1998676e-03,  1.3002789e-03,\n",
       "             1.5830012e-03,  1.2732974e-03,  1.4882013e-03,  6.5763830e-05,\n",
       "             1.1400300e-03,  3.8383296e-04,  1.5910043e-03,  1.4076765e-03,\n",
       "             1.2836594e-03,  1.3966571e-03,  1.3062084e-03,  1.3102929e-03,\n",
       "             1.4005117e-03,  1.7179702e-04,  1.1559662e-03,  1.4680903e-03,\n",
       "             1.3030489e-03,  1.3795467e-03,  7.0936274e-04,  3.9915464e-04,\n",
       "             1.6073907e-03,  1.6290215e-03,  2.6768196e-04,  1.2862000e-03,\n",
       "             5.2991987e-04,  1.5015837e-03,  1.6055164e-03,  1.4994668e-03,\n",
       "             1.4844937e-03,  1.6835461e-03,  1.5569587e-03,  1.4736149e-03,\n",
       "             1.4610423e-03,  4.2294746e-04,  1.6865230e-03,  1.0304712e-03,\n",
       "             1.3073902e-03,  1.4262747e-03,  1.5246843e-03,  1.4701716e-03,\n",
       "             1.1738428e-03,  1.2600599e-03,  1.5074379e-03,  1.1628723e-03,\n",
       "             7.0542231e-04,  1.3149360e-03,  1.2106438e-03,  1.4544897e-03,\n",
       "             1.3351005e-03,  1.4264757e-03,  1.5080663e-03,  1.3026233e-03,\n",
       "             2.5723351e-04,  1.2970748e-03,  1.3272640e-03,  1.5413263e-03,\n",
       "             1.1049996e-03,  1.6817073e-03,  1.6835646e-03,  1.7409906e-03,\n",
       "             1.7058057e-03,  1.5800205e-03,  1.1633975e-03,  1.4375929e-03,\n",
       "             1.0970740e-03,  1.2494270e-03,  1.2309632e-03,  1.5280782e-03,\n",
       "             1.1273170e-03,  1.3736343e-03,  1.5534465e-03,  3.9250302e-04,\n",
       "             1.5335489e-03,  1.3032013e-03,  1.1433709e-03,  1.2986626e-03,\n",
       "             1.6594792e-03,  1.5292735e-03,  1.6454960e-03,  1.6687942e-03,\n",
       "             1.3333209e-03,  7.8054040e-04,  9.8873151e-04,  1.3140080e-03,\n",
       "             1.7059486e-03,  1.3051619e-03,  1.1656316e-03,  1.3697695e-03,\n",
       "             9.8803011e-04,  1.0991393e-03,  1.2399835e-03,  1.4718366e-03,\n",
       "             1.2999248e-03,  1.3915439e-03,  1.4998119e-03,  1.4651286e-03,\n",
       "             1.5445852e-03,  1.2627067e-03,  1.2737506e-03,  1.6185751e-03,\n",
       "             1.5598008e-03,  1.5548599e-03,  1.0692580e-03,  1.4620255e-03,\n",
       "             1.5238859e-03,  1.3782580e-03,  6.5726216e-04,  8.8495685e-04,\n",
       "            -1.7425802e-03, -8.8190089e-04, -1.7182730e-03, -9.1206026e-04,\n",
       "            -1.9534486e-03, -1.9655204e-03, -8.2625641e-04, -4.5342644e-04,\n",
       "            -9.6985733e-04, -1.7800065e-03, -1.2173235e-03, -1.3000943e-03,\n",
       "            -5.3380714e-05,  3.5913772e-04, -5.7385361e-04, -8.6482120e-04,\n",
       "            -1.7832199e-03, -1.9821730e-03, -1.3887498e-03, -1.4196169e-03,\n",
       "            -1.8083684e-04, -1.2129850e-03, -1.8590804e-03, -4.5242597e-04,\n",
       "            -1.4594323e-03, -9.6777792e-04,  5.6835753e-04,  2.5254153e-04,\n",
       "            -9.9156366e-04,  3.9118735e-04, -1.9580917e-03, -7.4598682e-04,\n",
       "            -1.2222493e-04, -6.2949402e-04, -1.2192482e-03, -1.1241289e-03,\n",
       "            -1.7920301e-03, -1.1642574e-03, -1.5083215e-03, -5.5711565e-04,\n",
       "            -1.6901531e-03, -1.6774628e-03, -9.2517782e-04, -1.4196391e-03,\n",
       "            -9.1320404e-04, -1.3052544e-03, -9.6272206e-04, -1.9604957e-03,\n",
       "            -1.4689572e-03, -1.4327198e-03, -1.5960538e-03, -1.2660623e-03,\n",
       "            -7.6449412e-04, -1.4703922e-03, -9.3505019e-04, -1.1256788e-03,\n",
       "            -1.1716980e-03,  9.2466746e-04, -1.4869453e-03, -1.4171524e-03,\n",
       "            -1.6414603e-03, -1.3873047e-03, -1.5289865e-03, -4.6518224e-04,\n",
       "            -1.7876377e-03, -1.1266115e-03, -8.6431229e-04, -2.0401205e-03,\n",
       "            -1.3578788e-03, -1.8679034e-03, -8.6824235e-04, -1.9127578e-03,\n",
       "            -1.5060168e-03, -5.4017187e-04, -2.1444075e-03, -5.4058048e-04,\n",
       "             9.1311078e-05, -7.7675958e-04, -6.1853928e-04, -1.5157120e-03,\n",
       "            -2.2198258e-03, -2.5923373e-03, -5.4395339e-04,  1.3570422e-03,\n",
       "            -1.1964109e-03, -1.3334167e-03, -1.4472661e-03,  3.8547957e-04,\n",
       "            -1.9417403e-03, -1.1069450e-03, -1.0930381e-03, -1.1082666e-03,\n",
       "            -1.0751783e-03, -7.3169690e-04, -1.5809077e-03, -7.6462631e-04,\n",
       "            -1.1328326e-03, -1.3074098e-03, -9.4792718e-04, -2.7805191e-04,\n",
       "            -9.1594242e-04, -6.5282994e-04, -8.6955330e-04, -1.8435686e-03,\n",
       "            -7.6083484e-04, -1.4555134e-03, -2.3895337e-03, -1.2181323e-03,\n",
       "            -2.4745651e-04, -7.1196997e-04,  1.0308473e-04, -1.0305035e-03,\n",
       "            -8.5630774e-04, -1.5275464e-03, -4.8443669e-04, -1.7487912e-03,\n",
       "            -3.3965966e-04, -1.8654927e-03, -8.2434376e-04, -1.6717315e-03,\n",
       "            -1.0305016e-03, -2.0668986e-03, -1.9112304e-03, -1.3339516e-03,\n",
       "            -6.7280454e-04, -1.9231311e-03, -7.7676348e-04,  9.2898920e-04,\n",
       "            -1.1497418e-03, -1.1338689e-03, -1.6180540e-03, -1.5561170e-03,\n",
       "            -2.1017557e-03,  2.6140726e-04, -5.4364913e-04, -8.8943169e-04,\n",
       "            -1.4930295e-03,  1.8589836e-04, -1.6962705e-03, -5.4263516e-04,\n",
       "            -1.8507383e-03, -1.0116203e-03,  8.1420221e-05, -2.1787699e-04,\n",
       "            -9.6167938e-04,  8.1088260e-04, -7.8457757e-04, -1.3188329e-03,\n",
       "            -1.9046674e-03, -9.4757439e-04, -1.5158182e-03,  3.1174233e-04,\n",
       "             3.4175985e-04,  4.1362381e-04, -1.6185568e-03, -1.7539208e-03,\n",
       "            -1.6508534e-03, -1.5265819e-03, -6.5830135e-04, -6.4572453e-04,\n",
       "            -1.6423701e-03, -9.9016994e-04, -4.6375074e-04, -1.8735707e-03,\n",
       "            -1.4492458e-03, -9.6615229e-04, -1.2231116e-03, -3.2906019e-04,\n",
       "            -6.8136409e-04, -2.1131006e-03,  5.5415119e-04, -1.0765787e-03,\n",
       "            -6.0812681e-04, -1.8161372e-03, -1.0054382e-03, -1.2252396e-03,\n",
       "            -1.4360149e-03, -1.8819061e-03, -1.7057635e-03, -6.6781067e-04,\n",
       "            -4.2018932e-04,  1.2063660e-04, -2.3889036e-03, -5.6423852e-04,\n",
       "            -1.0372222e-03, -7.9851446e-04, -1.2185742e-03, -1.5191680e-03,\n",
       "             3.9903334e-04, -1.2290792e-03, -2.0159057e-03, -1.7270672e-03,\n",
       "            -8.6802564e-04, -6.6596572e-04, -3.0991741e-04, -1.3031537e-03,\n",
       "            -1.0292209e-03, -6.2632567e-04, -1.5754547e-03, -7.4590388e-04,\n",
       "             9.3943696e-04, -9.2087110e-04, -1.3413819e-03, -1.2372319e-03,\n",
       "            -9.9200336e-04, -1.7898962e-03, -1.7761621e-03, -1.6888360e-03,\n",
       "            -1.9985216e-03, -1.8366079e-03, -8.2815142e-04, -1.1091075e-03,\n",
       "            -6.0646198e-05, -4.1273620e-04, -1.3100847e-03, -2.0010024e-03,\n",
       "            -3.7110364e-04, -1.7147571e-03, -1.8364599e-03,  1.2456147e-03,\n",
       "            -1.1500565e-03, -1.6552805e-03, -1.7104660e-04, -5.5521540e-04,\n",
       "            -1.8842054e-03, -1.2292061e-03, -1.9576617e-03, -1.7143404e-03,\n",
       "            -2.8499163e-04,  1.6105602e-03,  7.0415763e-04, -8.6426071e-04,\n",
       "            -2.6265951e-03, -3.6172944e-04,  4.9553728e-05, -1.5562297e-03,\n",
       "             1.3891139e-04, -3.3098386e-04, -9.0197596e-04, -8.5674209e-04,\n",
       "            -1.0461167e-03, -1.1265767e-03, -1.7978183e-03, -7.5533363e-04,\n",
       "            -1.6173185e-03, -5.3903030e-04, -1.4249992e-03, -1.1040339e-03,\n",
       "            -1.2743875e-03, -1.4490569e-03, -6.2456593e-04, -9.2435745e-04,\n",
       "            -1.2614158e-03, -9.7666273e-04, -7.9248677e-04, -2.3314366e-04],\n",
       "           dtype=float32),\n",
       "     array([[-0.00425062,  0.00700617,  0.01413636, ...,  0.00567826,\n",
       "              0.00687482,  0.01112916],\n",
       "            [ 0.01003339, -0.01780564, -0.00358139, ...,  0.00179184,\n",
       "              0.00224039, -0.01780207],\n",
       "            [ 0.00877628,  0.00181289,  0.00349818, ...,  0.00220602,\n",
       "              0.00183543,  0.00594454],\n",
       "            ...,\n",
       "            [ 0.01019983, -0.00369361,  0.00753805, ...,  0.00560224,\n",
       "              0.00471552, -0.00069773],\n",
       "            [ 0.00854391,  0.01040975,  0.00168738, ..., -0.00895777,\n",
       "              0.00796059,  0.00504308],\n",
       "            [ 0.00204762, -0.00191399,  0.0065448 , ...,  0.00283446,\n",
       "             -0.00696476,  0.01318391]], dtype=float32),\n",
       "     array([ 0.00044728, -0.00893924, -0.00698331, -0.0077487 , -0.0082649 ,\n",
       "             0.00607179,  0.00728085,  0.0105443 , -0.00699519, -0.00590929,\n",
       "             0.00679032,  0.00716702,  0.00803537, -0.00910194,  0.00673216,\n",
       "             0.0078322 , -0.00960886,  0.0091236 , -0.0063251 , -0.00539157,\n",
       "             0.0083053 , -0.00817157, -0.00885331,  0.00993944, -0.0059902 ,\n",
       "             0.00725667,  0.00335591,  0.00799438, -0.01094173, -0.00793758,\n",
       "            -0.00610984,  0.00765141,  0.006077  , -0.00824808, -0.00800856,\n",
       "            -0.00666759,  0.00663156,  0.00809121,  0.00745219, -0.00784315,\n",
       "            -0.00755534, -0.00619026, -0.00615474,  0.00886078, -0.00788829,\n",
       "            -0.00605074, -0.0106209 , -0.00303812, -0.00857391, -0.00553098,\n",
       "             0.00404214, -0.00326867, -0.00751273, -0.00068589,  0.00721044,\n",
       "             0.00701992,  0.00245804,  0.01094406, -0.00115264,  0.00586197,\n",
       "             0.00414396,  0.00607814,  0.00184181,  0.00129868,  0.00915715,\n",
       "             0.00704479, -0.00697581,  0.01005565, -0.00698512,  0.00721217,\n",
       "            -0.00731798,  0.00672952, -0.0100443 , -0.00764775,  0.00611937,\n",
       "             0.00676574,  0.00834304, -0.00818921,  0.00811401, -0.0059199 ,\n",
       "            -0.00924292, -0.00670201, -0.00642075,  0.00615982, -0.00728458,\n",
       "            -0.00746532,  0.00687945, -0.00721455,  0.0062301 ,  0.00718369,\n",
       "            -0.00651505, -0.0076578 , -0.00701368,  0.00797609,  0.00558845,\n",
       "            -0.00729479,  0.00732861, -0.00779837,  0.00489897, -0.00746086,\n",
       "            -0.00722805, -0.00047558,  0.00657629,  0.00803136, -0.00756481,\n",
       "             0.00751529,  0.00707655, -0.00702284, -0.00825013,  0.00697474,\n",
       "             0.00772803,  0.0077125 , -0.00709999, -0.0080183 , -0.01155966,\n",
       "            -0.00574931,  0.01073295,  0.01003565, -0.00701031, -0.0065726 ,\n",
       "            -0.00755097,  0.0078944 , -0.00925753,  0.00662363, -0.00743725,\n",
       "            -0.00698864, -0.00738971,  0.0069921 , -0.0067948 , -0.00741636,\n",
       "            -0.00822532,  0.00294005,  0.00733371, -0.01043331,  0.00726363,\n",
       "            -0.00566913,  0.00749644,  0.00783605,  0.00701573, -0.00620024,\n",
       "             0.00869111,  0.00709393, -0.00804677,  0.00527238, -0.0071785 ,\n",
       "             0.00883966, -0.0075207 ,  0.00446917,  0.00954174,  0.00748528,\n",
       "            -0.00659885,  0.01054588,  0.00818935, -0.00933966, -0.00788005,\n",
       "            -0.01067396,  0.0103915 , -0.00557927,  0.00757466,  0.00757109,\n",
       "            -0.00714103, -0.00027099, -0.00855385, -0.0095079 , -0.00724404,\n",
       "            -0.00725305, -0.00085068, -0.00712974,  0.00673206, -0.00740341,\n",
       "             0.00029882, -0.00719736,  0.00117253, -0.00866056,  0.00683766,\n",
       "            -0.00980613, -0.00712988,  0.00853375,  0.00677732,  0.00741245,\n",
       "             0.00742132,  0.00035785,  0.0085891 , -0.00855602,  0.00771129,\n",
       "             0.00726746, -0.00606881,  0.00674844,  0.00771217, -0.00793298,\n",
       "             0.00940319,  0.00189109, -0.00142109,  0.00761236, -0.00809916,\n",
       "            -0.00666087,  0.00726734,  0.00749598, -0.00555556,  0.0074526 ,\n",
       "            -0.00376315,  0.00737033,  0.0073897 , -0.00704262, -0.0083614 ,\n",
       "            -0.00702564, -0.00694726,  0.0061522 , -0.00618017, -0.00919385,\n",
       "             0.00788352, -0.00749962,  0.00793448, -0.00816628,  0.0073411 ,\n",
       "            -0.00610063, -0.00782175, -0.00734844, -0.00580754, -0.00203555,\n",
       "            -0.00597752, -0.01093025,  0.010155  ,  0.00756543, -0.00721202,\n",
       "            -0.00796422,  0.0081084 , -0.00681223, -0.00789387,  0.01048738,\n",
       "            -0.01133768,  0.00732476,  0.00698756, -0.00918576,  0.00883596,\n",
       "             0.00784017, -0.01045916, -0.00803591,  0.00749793,  0.00590429,\n",
       "            -0.00764284, -0.00815597, -0.00719212, -0.00717529,  0.00717757,\n",
       "             0.007549  , -0.00771659, -0.00697058, -0.00611469, -0.00659213,\n",
       "            -0.00802077, -0.00718109, -0.00595124, -0.01068923, -0.00255337,\n",
       "            -0.00857523], dtype=float32)),),\n",
       "   ()),\n",
       "  (((), ((), ())),\n",
       "   ((array([[ 0.00387193,  0.00123309, -0.0016599 , ..., -0.00587204,\n",
       "              0.00101223, -0.00094377],\n",
       "            [ 0.00678395,  0.008142  , -0.00524549, ..., -0.0084337 ,\n",
       "             -0.00129028,  0.00466194],\n",
       "            [-0.00535855, -0.00674197, -0.0041627 , ...,  0.0016527 ,\n",
       "              0.00661777,  0.00017609],\n",
       "            ...,\n",
       "            [ 0.00912821,  0.00813061,  0.00435458, ..., -0.00619057,\n",
       "              0.00253182,  0.00683461],\n",
       "            [ 0.00217302, -0.00325569,  0.0052338 , ..., -0.00291523,\n",
       "              0.00478653, -0.00111674],\n",
       "            [-0.00312083, -0.00342285, -0.00794189, ..., -0.0086534 ,\n",
       "             -0.00268179, -0.00797287]], dtype=float32),\n",
       "     array([ 3.0133457e-05,  6.6120940e-04,  1.3644850e-03,  1.2533737e-03,\n",
       "             1.1933033e-03,  2.8758880e-04,  6.4604421e-04,  1.4347375e-03,\n",
       "             1.4035680e-03,  1.2145389e-03,  1.3679740e-03,  1.0344126e-03,\n",
       "             1.3264116e-03,  1.4723912e-03,  1.2573443e-03,  1.0268585e-03,\n",
       "             1.4183299e-03,  1.1912986e-03,  1.3091606e-03,  7.3486421e-04,\n",
       "             7.9842395e-04,  1.3009448e-03,  1.0904025e-03,  8.9579576e-04,\n",
       "             1.3843802e-03,  1.4336045e-03,  3.6315803e-04,  1.2756016e-03,\n",
       "             9.8904187e-04,  1.2804928e-03,  8.6413790e-04,  1.0818579e-03,\n",
       "             1.1964744e-03,  1.4847700e-03,  1.2289304e-03,  1.2375265e-03,\n",
       "             1.5641495e-03,  1.3781117e-03,  1.2469774e-03,  1.3197327e-03,\n",
       "             1.1472724e-03,  1.5756239e-03,  9.3346927e-04,  1.3882032e-03,\n",
       "             1.3200678e-03,  8.5947395e-04,  1.3215869e-03,  1.4082439e-03,\n",
       "             1.0051246e-03,  1.1945838e-03,  1.3647005e-03,  1.0346374e-03,\n",
       "             1.2429383e-03,  1.2777218e-03,  4.1270009e-04,  1.4259884e-03,\n",
       "             4.1334526e-04,  1.3246512e-03,  6.2681618e-04,  1.2157629e-03,\n",
       "             8.5753715e-04,  1.3940155e-03,  1.5084515e-03,  1.2661854e-03,\n",
       "             9.6781092e-04,  1.0494576e-03,  1.2876441e-03,  7.7287958e-05,\n",
       "             1.1580869e-03,  3.8733424e-04,  1.3830866e-03,  1.3908284e-03,\n",
       "             4.7637950e-04,  1.3796340e-03,  1.2858325e-03,  1.2736724e-03,\n",
       "             2.6882740e-04,  1.2984619e-03,  1.1091137e-03,  1.2499705e-03,\n",
       "             1.3135476e-03,  6.5262523e-04,  1.3564414e-03,  1.2159552e-03,\n",
       "             1.8563672e-04,  1.2501179e-03,  6.2468687e-05,  1.3533719e-03,\n",
       "             1.3777950e-03,  4.8694780e-04,  1.4290643e-03,  1.4198418e-03,\n",
       "             1.5565498e-03,  1.0876923e-03,  2.0694391e-04,  8.9560577e-04,\n",
       "             1.0153380e-03,  1.2431573e-03,  1.3524374e-03,  1.2186334e-03,\n",
       "             6.5431354e-04,  9.8999299e-04,  1.6122566e-03,  7.0691883e-04,\n",
       "             1.3542295e-03,  1.4320441e-03,  1.3837767e-03,  1.3010941e-03,\n",
       "             2.7783474e-04,  1.4025635e-03,  1.2769537e-03,  1.3323379e-03,\n",
       "             1.1025507e-03,  1.3229249e-03,  1.2616579e-03,  9.1165316e-04,\n",
       "             1.3165137e-03,  1.6659056e-03,  1.0536569e-03,  1.3514889e-03,\n",
       "             1.1338157e-03,  1.0479074e-03,  1.1338795e-03,  1.3912037e-03,\n",
       "             9.0789013e-05,  1.4728410e-03,  1.0625881e-03,  1.4409217e-03,\n",
       "             8.7201559e-05,  1.3746017e-03,  1.0746003e-03,  1.5418959e-03,\n",
       "             1.2325175e-03,  1.2248501e-03,  8.7807322e-04,  1.2942633e-03,\n",
       "             1.2571604e-03,  1.3175323e-03,  1.3352408e-03,  1.4298110e-03,\n",
       "             1.0685909e-03,  1.0782193e-03,  3.9064398e-04,  7.1900868e-04,\n",
       "             1.0963531e-03,  9.8460936e-04,  9.8765595e-05,  1.2240416e-03,\n",
       "             1.3444633e-03,  5.6634960e-04,  1.4820980e-03,  1.0695391e-03,\n",
       "             1.4061345e-03,  1.2188912e-03,  1.1978444e-03,  1.4751274e-03,\n",
       "             6.8565118e-06,  5.9569697e-04,  1.3563413e-03,  1.3860677e-03,\n",
       "             5.2635220e-04,  1.4263625e-03,  1.2485135e-03,  1.3635671e-03,\n",
       "             1.5996710e-03,  1.1768192e-03,  1.3205251e-03,  1.3723642e-03,\n",
       "             1.4151133e-03,  3.0152107e-04,  1.4175329e-03,  1.2888458e-03,\n",
       "             1.2406072e-03,  1.4322617e-03,  1.3503483e-03,  1.2792771e-03,\n",
       "             1.0898500e-03,  1.2200228e-03,  8.9834910e-04,  1.1826617e-03,\n",
       "             1.3010092e-03,  1.1276272e-03,  1.4454506e-03,  1.4730481e-03,\n",
       "             6.1914057e-04,  1.2035719e-03,  8.8147557e-04,  1.3060708e-03,\n",
       "             1.4986313e-03,  8.3815627e-04,  1.3889223e-03,  1.2232646e-03,\n",
       "             1.3790780e-04,  1.3170973e-03,  1.5188726e-03,  1.0203155e-03,\n",
       "             6.2717195e-04,  1.3136242e-03,  1.3793419e-03,  1.3743436e-03,\n",
       "             1.4215385e-03,  1.4797766e-03,  1.2864609e-03,  1.2334043e-03,\n",
       "             1.4746273e-03,  1.0074261e-03,  1.4860983e-03,  1.3583648e-03,\n",
       "             1.2907706e-03,  1.1092916e-03,  1.2177451e-03,  1.5580283e-03,\n",
       "             6.4104359e-04,  1.2131549e-03,  1.2536958e-03,  1.2714393e-03,\n",
       "             1.0926818e-03,  9.9722494e-04,  1.3831754e-03,  1.1756946e-03,\n",
       "             9.8485127e-04,  3.1151107e-04,  1.2600995e-03,  1.0100900e-03,\n",
       "             1.1033842e-03,  1.4240071e-03,  7.9596532e-05,  8.4814971e-04,\n",
       "             1.2302577e-03,  1.3938473e-03,  1.3637682e-03,  1.3087014e-03,\n",
       "             3.6419168e-04,  1.2977704e-03,  1.2514925e-03,  1.2393639e-03,\n",
       "             1.2595720e-03,  1.1172929e-03,  4.3472795e-05,  1.1642808e-03,\n",
       "             1.1212993e-03,  1.4745883e-03,  1.2896590e-04,  1.0726212e-03,\n",
       "             1.2158671e-03,  1.3435041e-03,  1.2740125e-03,  1.2461584e-03,\n",
       "             1.3558645e-03,  9.9241349e-04,  1.1408279e-03,  1.2292009e-03,\n",
       "             1.4976059e-03,  1.4700616e-03,  1.0941959e-03,  1.2299075e-03,\n",
       "            -2.1908313e-06, -1.1082701e-03, -2.1549307e-03, -1.2168596e-03,\n",
       "            -8.4594707e-04, -8.1165822e-04, -8.4420230e-04, -1.8079054e-03,\n",
       "            -1.1799906e-03, -1.2211044e-03, -1.4378369e-03, -1.1813018e-03,\n",
       "            -7.8651152e-04, -1.2938420e-03, -1.0887805e-03, -9.9988980e-04,\n",
       "            -1.1750234e-03, -9.1787631e-04, -1.3869543e-03, -1.1269092e-03,\n",
       "            -1.3061306e-03, -9.2896092e-04, -1.1854431e-03, -1.4358078e-03,\n",
       "            -1.3660796e-03, -1.2292642e-03, -7.9858187e-04, -1.1021090e-03,\n",
       "            -1.0165343e-03, -1.7717022e-03, -9.7855006e-04, -1.3014454e-03,\n",
       "            -8.6037221e-04, -1.1567272e-03, -1.5949864e-03, -1.0537248e-03,\n",
       "            -1.6045785e-03, -1.6481353e-03, -1.5635758e-03, -1.4819667e-03,\n",
       "            -1.0681449e-03, -1.5314925e-03, -9.5170946e-04, -1.6283713e-03,\n",
       "            -6.9655100e-04, -1.2622346e-03, -1.3001697e-03, -1.3697938e-03,\n",
       "            -1.4267694e-03, -2.0226935e-04, -1.6907655e-03, -9.6898584e-04,\n",
       "            -1.4516708e-03, -1.2737100e-03, -9.0811803e-04, -4.9592432e-04,\n",
       "            -8.9250586e-04, -1.7412553e-03, -8.7854790e-04, -1.4467605e-03,\n",
       "            -1.3661812e-03, -1.2236114e-03, -1.5229604e-03, -1.0035884e-03,\n",
       "            -1.6692914e-03, -1.5033896e-03, -7.7467615e-04, -2.3039256e-04,\n",
       "            -1.2971332e-03, -8.2917081e-04, -1.3095667e-03, -9.2838961e-04,\n",
       "            -8.8985637e-04, -7.4686785e-04, -9.8301354e-04, -1.5565678e-03,\n",
       "            -6.4260093e-04, -1.9643581e-03, -1.0983963e-03, -1.1307409e-03,\n",
       "            -9.4386429e-04, -6.2054378e-04, -9.3580916e-04, -1.3413483e-03,\n",
       "            -5.0417648e-04, -1.5549506e-03, -1.3402160e-04, -1.5966592e-03,\n",
       "            -1.7742126e-03, -1.0850605e-03, -1.6996233e-03, -1.0025408e-03,\n",
       "            -2.0369866e-03, -1.3252364e-03, -4.2649108e-04, -1.0901357e-03,\n",
       "            -1.3413944e-03, -1.8149999e-03, -1.7685387e-03, -8.9577830e-04,\n",
       "            -1.1311959e-03, -1.1142978e-03, -1.4332347e-03, -1.1718239e-03,\n",
       "            -1.6532175e-03, -1.4241626e-03, -9.4728050e-04, -1.1930650e-03,\n",
       "            -5.9338677e-04, -1.0013902e-03, -1.1672789e-03, -1.0805050e-03,\n",
       "            -1.2862697e-03, -1.0106720e-03, -7.2467100e-04, -9.1298332e-04,\n",
       "            -8.7874534e-04, -2.2280102e-03, -1.5373133e-03, -1.0010406e-03,\n",
       "            -1.4378435e-03, -1.5704668e-03, -1.5943479e-03, -1.6344765e-03,\n",
       "             2.9898906e-04, -1.3743447e-03, -1.5566254e-03, -2.1933322e-03,\n",
       "            -1.3443295e-04, -1.7574629e-03, -1.0269379e-03, -1.7194769e-03,\n",
       "            -1.1898945e-03, -8.7704079e-04, -1.0946930e-03, -1.3871714e-03,\n",
       "            -1.4520317e-03, -1.1343750e-03, -6.6912698e-04, -1.8342731e-03,\n",
       "            -1.3387599e-03, -1.3478207e-03, -8.4112067e-04, -1.1009688e-03,\n",
       "            -1.4820374e-03, -1.6249674e-03, -2.4129309e-04, -1.7093178e-03,\n",
       "            -1.4937768e-03, -9.8053238e-04, -1.4484260e-03, -1.2653642e-03,\n",
       "            -1.4050789e-03, -1.3035564e-03, -1.0548948e-03, -1.2189357e-03,\n",
       "            -1.9622788e-04, -1.0660833e-03, -1.6410649e-03, -1.4216622e-03,\n",
       "            -8.9355448e-04, -1.0766212e-03, -9.9695951e-04, -2.2401975e-03,\n",
       "            -8.9384615e-04, -1.1382963e-03, -1.0813698e-03, -1.6696294e-03,\n",
       "            -1.3141801e-03, -6.8193791e-04, -1.2890823e-03, -5.4293941e-04,\n",
       "            -1.4815433e-03, -1.0866484e-03, -1.6951816e-03, -9.9155272e-04,\n",
       "            -1.1311319e-03, -9.4735634e-04, -1.1343220e-03, -1.3360906e-03,\n",
       "            -1.3943962e-03, -1.1282316e-03, -1.3809067e-03, -1.5416711e-03,\n",
       "            -1.0928619e-03, -1.6488120e-03, -1.1593745e-03, -1.2176202e-03,\n",
       "            -1.4831056e-03, -1.3513196e-03, -1.6754435e-03, -2.1533996e-03,\n",
       "            -3.6379151e-04, -8.2206452e-04, -1.6903165e-03, -1.6461281e-03,\n",
       "            -1.0012990e-03, -1.4609220e-03, -1.3011115e-03, -1.0387689e-03,\n",
       "            -1.3155864e-03, -1.1229791e-03, -1.5655684e-03, -9.9181349e-04,\n",
       "            -1.2541228e-03, -1.4041051e-03, -9.2678040e-04, -7.9563039e-04,\n",
       "            -1.5052895e-03, -9.0167951e-04, -1.2495440e-03, -1.5511246e-03,\n",
       "            -1.0833780e-03, -9.7820000e-04, -1.3975919e-03, -1.0606558e-03,\n",
       "            -9.0813421e-04, -1.5547994e-03, -8.5618370e-04, -1.5978643e-03,\n",
       "            -1.5027374e-03, -7.5305579e-04, -1.0086192e-03, -1.4281692e-03,\n",
       "            -1.4433757e-03, -7.9217163e-04, -2.9414363e-04, -9.1634883e-04,\n",
       "            -1.4587636e-03, -1.2110071e-03, -1.0276104e-03, -1.6466535e-03,\n",
       "            -8.2366716e-04, -1.0270462e-03, -1.6548149e-03, -1.0525045e-03,\n",
       "            -1.0754098e-03, -8.5547141e-04, -1.7310378e-04, -1.3665411e-03,\n",
       "            -1.0241673e-03, -1.6286664e-03, -3.0446993e-04, -6.8104145e-04,\n",
       "            -1.5652108e-03, -1.5306700e-03, -1.5476322e-03, -1.3245447e-03,\n",
       "            -1.5340651e-03, -1.1584108e-03, -1.1292867e-03, -1.6156170e-03,\n",
       "            -1.4791867e-03, -1.0959693e-03, -1.5704629e-03, -1.3341390e-03],\n",
       "           dtype=float32),\n",
       "     array([[ 0.00390511,  0.01014089,  0.00023875, ..., -0.00669696,\n",
       "              0.00412932,  0.00617987],\n",
       "            [-0.00462444,  0.00101785, -0.00312658, ..., -0.00500585,\n",
       "             -0.01252878,  0.01202962],\n",
       "            [-0.00513906,  0.00984123,  0.00293713, ..., -0.00948377,\n",
       "             -0.0041041 ,  0.01358745],\n",
       "            ...,\n",
       "            [-0.00367025, -0.00191842,  0.01071893, ..., -0.00670014,\n",
       "              0.00495785, -0.00991529],\n",
       "            [ 0.00208391, -0.00604478, -0.0085802 , ..., -0.00088735,\n",
       "             -0.00208041,  0.00557846],\n",
       "            [-0.0072357 , -0.00929135,  0.00831005, ..., -0.00332468,\n",
       "             -0.0099759 , -0.0066385 ]], dtype=float32),\n",
       "     array([ 0.00760121, -0.00786075,  0.00806712, -0.00805867,  0.00814564,\n",
       "             0.00761107, -0.00752752,  0.00797251,  0.00799994,  0.00802105,\n",
       "             0.0080162 , -0.00811129, -0.00801193, -0.00799862, -0.0080114 ,\n",
       "            -0.00818919,  0.00798212,  0.00795077, -0.00798351,  0.00788969,\n",
       "             0.00790997,  0.00806155, -0.00794628,  0.00789265, -0.00802109,\n",
       "             0.00800223,  0.00791898,  0.00800877, -0.00804728, -0.00805358,\n",
       "            -0.00779235, -0.00785841,  0.00798964, -0.00797987, -0.00805433,\n",
       "             0.00803436,  0.00796727,  0.00801456, -0.0080233 , -0.00804378,\n",
       "            -0.00818243, -0.00797356, -0.00802124, -0.00802183, -0.00802743,\n",
       "             0.00797361, -0.00802122,  0.00800562,  0.00798692,  0.00811678,\n",
       "            -0.00803457, -0.00823288,  0.00801126,  0.00804514, -0.0076961 ,\n",
       "             0.00799121, -0.00690945,  0.00803447,  0.00714638,  0.00809001,\n",
       "             0.00805395,  0.00801349, -0.00795678, -0.00804966, -0.00823056,\n",
       "             0.00802252, -0.00803677, -0.00539334, -0.00798767,  0.00702459,\n",
       "            -0.00800798,  0.00802274, -0.00752289,  0.00799132, -0.00803289,\n",
       "             0.00805932, -0.00830641,  0.0080771 , -0.00793428, -0.00800082,\n",
       "            -0.00803187, -0.00819489, -0.00802347, -0.00799188, -0.00740798,\n",
       "             0.00804967,  0.00624352,  0.00804992,  0.00802378, -0.00751302,\n",
       "             0.00800241,  0.00799289,  0.00796654, -0.00801189, -0.0075757 ,\n",
       "            -0.00810098, -0.00790655,  0.00807383, -0.00803272,  0.00806149,\n",
       "            -0.00783508,  0.00795517, -0.00795505, -0.00767894,  0.00804511,\n",
       "             0.00801567,  0.00800979,  0.00803529,  0.00727144,  0.00802099,\n",
       "             0.00806827,  0.0080329 , -0.00817758,  0.00803179, -0.00800366,\n",
       "             0.00786196, -0.00804423,  0.00796535, -0.0079891 ,  0.00803818,\n",
       "            -0.00797122, -0.00801806, -0.00806118, -0.00800134, -0.00767308,\n",
       "            -0.00801239,  0.00800282, -0.00800973,  0.00689354,  0.00796847,\n",
       "             0.0080549 ,  0.00797677, -0.00804611,  0.00807682,  0.00796285,\n",
       "            -0.00805319, -0.00803596,  0.00800943,  0.00803162,  0.00801031,\n",
       "             0.0079711 , -0.00792412,  0.00704953,  0.00781735, -0.0080239 ,\n",
       "            -0.00800613, -0.00552571,  0.00810886,  0.00801434,  0.00747824,\n",
       "             0.00799692,  0.0080358 ,  0.00802048,  0.00806614,  0.00798728,\n",
       "            -0.00796133, -0.00084822,  0.00763757, -0.00802889, -0.00796656,\n",
       "            -0.00773454,  0.00801446,  0.00805793, -0.00803611, -0.00797203,\n",
       "             0.00808115, -0.00803549, -0.00801804,  0.00800043, -0.00695427,\n",
       "            -0.00796241, -0.00801592,  0.00799678, -0.00798359,  0.00799897,\n",
       "            -0.00803059,  0.00803777,  0.00794625, -0.00809352,  0.00802039,\n",
       "             0.00802498, -0.00797976,  0.00798261,  0.00799555, -0.00774066,\n",
       "             0.00802716, -0.00820484, -0.00806519,  0.00798312, -0.00782262,\n",
       "             0.00797253, -0.00811088, -0.0076323 , -0.0079859 ,  0.00799009,\n",
       "             0.00804944, -0.00815725,  0.00807572, -0.00806298, -0.00803105,\n",
       "            -0.00801244,  0.00797871,  0.00804227,  0.00809577, -0.00795834,\n",
       "            -0.00803716, -0.00796051, -0.00800271, -0.00803781,  0.00808103,\n",
       "            -0.0080396 ,  0.00795831,  0.00788359, -0.00805745, -0.00806401,\n",
       "            -0.00803251,  0.00791606, -0.00797268, -0.00801023, -0.00811161,\n",
       "             0.0078382 ,  0.00747777,  0.00807783, -0.00807631,  0.0080869 ,\n",
       "            -0.00800549,  0.00521538,  0.00771297,  0.00802584, -0.00801921,\n",
       "            -0.00802361,  0.00803608, -0.0060105 , -0.00802983,  0.00802896,\n",
       "            -0.00799989,  0.00804607, -0.00813676,  0.00504201, -0.00797776,\n",
       "             0.00804817, -0.00800922,  0.00785653,  0.00796228,  0.00798578,\n",
       "            -0.0080328 , -0.00801336, -0.00806832, -0.00803891, -0.00784095,\n",
       "            -0.00803654, -0.00807801, -0.00800595,  0.0079525 ,  0.00799531,\n",
       "            -0.00804482], dtype=float32)),),\n",
       "   ()),\n",
       "  (),\n",
       "  (array([[-0.05255705, -0.06514563],\n",
       "          [-0.11588445, -0.09691809],\n",
       "          [ 0.03992333, -0.1066009 ],\n",
       "          [-0.02120797,  0.13174945],\n",
       "          [-0.06499574, -0.12417889],\n",
       "          [-0.04822872, -0.05921659],\n",
       "          [ 0.00578554,  0.01306449],\n",
       "          [ 0.11241188, -0.13488965],\n",
       "          [ 0.10075278, -0.0878872 ],\n",
       "          [ 0.046569  , -0.01643222],\n",
       "          [ 0.13663402, -0.08206666],\n",
       "          [-0.11931106, -0.03153655],\n",
       "          [-0.10816363,  0.08191644],\n",
       "          [-0.02956916,  0.14829282],\n",
       "          [ 0.0024124 ,  0.08306431],\n",
       "          [ 0.07221106,  0.11935132],\n",
       "          [ 0.13353142, -0.07656774],\n",
       "          [ 0.12478985,  0.07222687],\n",
       "          [-0.14963555,  0.10937464],\n",
       "          [-0.09599912, -0.11869588],\n",
       "          [ 0.04015043, -0.01555332],\n",
       "          [ 0.07282372, -0.04345639],\n",
       "          [-0.00387318,  0.03625343],\n",
       "          [-0.06219922, -0.10378467],\n",
       "          [-0.04089427,  0.09008411],\n",
       "          [ 0.05438978, -0.10571585],\n",
       "          [ 0.01238411,  0.00183361],\n",
       "          [-0.01958047, -0.09242868],\n",
       "          [-0.04099496, -0.00071918],\n",
       "          [-0.03371999,  0.11753914],\n",
       "          [ 0.04993884,  0.06963182],\n",
       "          [-0.06076726, -0.0243486 ],\n",
       "          [-0.0656893 , -0.11544361],\n",
       "          [-0.03379308,  0.15230186],\n",
       "          [-0.03647594,  0.04859684],\n",
       "          [-0.05690522, -0.13840608],\n",
       "          [ 0.12310066, -0.12978916],\n",
       "          [ 0.0818406 , -0.09110132],\n",
       "          [ 0.04211839,  0.13449666],\n",
       "          [-0.11388563,  0.03345064],\n",
       "          [ 0.09039148,  0.15313525],\n",
       "          [-0.11069769,  0.10136497],\n",
       "          [-0.04291001, -0.02493348],\n",
       "          [ 0.02766998,  0.14881903],\n",
       "          [-0.00070405,  0.13339451],\n",
       "          [-0.03267504, -0.0688428 ],\n",
       "          [-0.11247803, -0.0212843 ],\n",
       "          [ 0.15138592, -0.02088872],\n",
       "          [ 0.03849247, -0.01041061],\n",
       "          [ 0.01949747, -0.0983924 ],\n",
       "          [-0.09321596,  0.04146047],\n",
       "          [-0.00337131,  0.03372592],\n",
       "          [-0.02946026, -0.10858741],\n",
       "          [ 0.11757573,  0.00055249],\n",
       "          [ 0.0435198 ,  0.0647717 ],\n",
       "          [ 0.1551556 ,  0.01089921],\n",
       "          [ 0.02293512,  0.02170429],\n",
       "          [ 0.1488336 ,  0.01697606],\n",
       "          [ 0.06756352,  0.06240268],\n",
       "          [ 0.03629586, -0.03744053],\n",
       "          [-0.09230736, -0.12970668],\n",
       "          [ 0.06871042, -0.05419882],\n",
       "          [-0.13619302,  0.12370238],\n",
       "          [ 0.0733391 ,  0.1489148 ],\n",
       "          [ 0.03879633,  0.09249179],\n",
       "          [-0.06843434, -0.12485144],\n",
       "          [ 0.02918231,  0.13529778],\n",
       "          [-0.03202166, -0.02955196],\n",
       "          [-0.13466054,  0.15280525],\n",
       "          [ 0.09592076,  0.0954873 ],\n",
       "          [ 0.02746197,  0.14965403],\n",
       "          [ 0.02854117, -0.08526213],\n",
       "          [-0.10262452, -0.09505871],\n",
       "          [ 0.03888472, -0.14635962],\n",
       "          [-0.06671845,  0.08901003],\n",
       "          [ 0.14187188,  0.03372094],\n",
       "          [-0.10590401, -0.09691302],\n",
       "          [ 0.08754413, -0.01809739],\n",
       "          [ 0.08053448,  0.12220722],\n",
       "          [ 0.03274836,  0.09512255],\n",
       "          [-0.01142856,  0.11949472],\n",
       "          [ 0.02836697,  0.07584833],\n",
       "          [-0.00261392,  0.10369328],\n",
       "          [-0.02463714,  0.03968592],\n",
       "          [ 0.02518085,  0.02886207],\n",
       "          [ 0.12783344,  0.02987577],\n",
       "          [-0.08043054, -0.09029931],\n",
       "          [ 0.10676469, -0.03687746],\n",
       "          [ 0.03991634, -0.11227738],\n",
       "          [-0.00601464,  0.01093445],\n",
       "          [ 0.12929253, -0.06307626],\n",
       "          [ 0.0299325 , -0.15304945],\n",
       "          [ 0.1131317 , -0.10707583],\n",
       "          [-0.03834996,  0.01993728],\n",
       "          [-0.00402943,  0.00638081],\n",
       "          [-0.05377883, -0.01984857],\n",
       "          [-0.07859898, -0.03515131],\n",
       "          [ 0.1457531 ,  0.05001057],\n",
       "          [-0.07466456,  0.08113804],\n",
       "          [ 0.05265209, -0.01556985],\n",
       "          [-0.13657834, -0.11814833],\n",
       "          [-0.03460126, -0.07227739],\n",
       "          [-0.1273816 ,  0.14521483],\n",
       "          [-0.14076836, -0.12117974],\n",
       "          [-0.00496276, -0.11799163],\n",
       "          [ 0.09483812, -0.0645965 ],\n",
       "          [ 0.07652654, -0.04667306],\n",
       "          [-0.00211167, -0.1031265 ],\n",
       "          [ 0.03098416,  0.04144957],\n",
       "          [ 0.0123733 , -0.12868755],\n",
       "          [ 0.1018547 ,  0.01186855],\n",
       "          [ 0.02076444, -0.1281893 ],\n",
       "          [ 0.06451347,  0.11914173],\n",
       "          [-0.00182985, -0.1470728 ],\n",
       "          [-0.14743946,  0.05899961],\n",
       "          [ 0.08486839,  0.06374413],\n",
       "          [-0.01007267,  0.1132486 ],\n",
       "          [ 0.13444479, -0.12245601],\n",
       "          [-0.00612429,  0.05486062],\n",
       "          [ 0.03984856, -0.06815821],\n",
       "          [-0.04288874,  0.0174967 ],\n",
       "          [-0.06187415,  0.00140525],\n",
       "          [-0.00923159,  0.05708702],\n",
       "          [-0.07846045,  0.1146771 ],\n",
       "          [ 0.08063385,  0.10660966],\n",
       "          [-0.01139103,  0.13095365],\n",
       "          [-0.05715899, -0.1206601 ],\n",
       "          [-0.04631281,  0.13390715],\n",
       "          [ 0.03706687,  0.02600161],\n",
       "          [ 0.14610305, -0.13779952],\n",
       "          [ 0.06479386,  0.02062186],\n",
       "          [ 0.1272977 , -0.09364569],\n",
       "          [-0.13267022, -0.06763287],\n",
       "          [ 0.07848601,  0.01073548],\n",
       "          [ 0.07473637,  0.04733248],\n",
       "          [-0.05008895,  0.10780874],\n",
       "          [-0.03640001,  0.05536372],\n",
       "          [ 0.14426418,  0.05916274],\n",
       "          [ 0.11180276, -0.03431803],\n",
       "          [ 0.02274877, -0.14894676],\n",
       "          [-0.02137236, -0.06432254],\n",
       "          [-0.04830302,  0.0042569 ],\n",
       "          [-0.01705882, -0.02926128],\n",
       "          [-0.05064509, -0.07496046],\n",
       "          [-0.03404656,  0.01750062],\n",
       "          [-0.07932049, -0.0243393 ],\n",
       "          [ 0.07848202,  0.08425128],\n",
       "          [ 0.05039148, -0.04202869],\n",
       "          [ 0.15355027, -0.01945734],\n",
       "          [-0.10472971, -0.12172653],\n",
       "          [ 0.02320756, -0.12486906],\n",
       "          [ 0.04686357, -0.00423162],\n",
       "          [ 0.1011366 , -0.07289823],\n",
       "          [ 0.1191203 , -0.01426098],\n",
       "          [-0.07052138, -0.13011126],\n",
       "          [-0.14899471,  0.12687646],\n",
       "          [ 0.10676844,  0.11995827],\n",
       "          [ 0.07367425,  0.05782154],\n",
       "          [-0.11420623,  0.0150787 ],\n",
       "          [-0.11588244,  0.14431225],\n",
       "          [-0.11662917, -0.09376022],\n",
       "          [ 0.05601773, -0.10015589],\n",
       "          [-0.01988166, -0.11905802],\n",
       "          [-0.14467642, -0.0119924 ],\n",
       "          [-0.08098966,  0.12476841],\n",
       "          [ 0.11120331,  0.03737596],\n",
       "          [-0.00142249,  0.10041513],\n",
       "          [ 0.00889325,  0.13443479],\n",
       "          [ 0.10689132, -0.08689857],\n",
       "          [ 0.1372248 ,  0.14841464],\n",
       "          [-0.1526417 ,  0.13233998],\n",
       "          [-0.07592462,  0.07396463],\n",
       "          [ 0.05602995, -0.01157841],\n",
       "          [-0.08918852,  0.05650358],\n",
       "          [ 0.1180286 , -0.10574531],\n",
       "          [-0.09526249,  0.09148522],\n",
       "          [ 0.11761316,  0.07368246],\n",
       "          [ 0.15434793,  0.10714999],\n",
       "          [-0.14887789, -0.12077969],\n",
       "          [ 0.00249831, -0.06059933],\n",
       "          [ 0.00218365, -0.11150218],\n",
       "          [ 0.04923071,  0.09675528],\n",
       "          [ 0.12497838, -0.09265015],\n",
       "          [ 0.05942614, -0.09843229],\n",
       "          [ 0.06890623,  0.08958462],\n",
       "          [ 0.12789631,  0.04999108],\n",
       "          [-0.03735754, -0.01041381],\n",
       "          [-0.09527791, -0.00676089],\n",
       "          [ 0.07590488, -0.14747252],\n",
       "          [ 0.05401109,  0.08892532],\n",
       "          [ 0.12258039, -0.09244612],\n",
       "          [ 0.02443308,  0.12093884],\n",
       "          [-0.10115933, -0.09005822],\n",
       "          [-0.05980784, -0.00072139],\n",
       "          [ 0.11991071, -0.09232489],\n",
       "          [-0.05770209, -0.11457905],\n",
       "          [-0.11265453, -0.09238888],\n",
       "          [ 0.1168152 , -0.03230246],\n",
       "          [-0.12291531, -0.01219403],\n",
       "          [-0.00208045,  0.1283802 ],\n",
       "          [-0.13871689,  0.07279038],\n",
       "          [ 0.15328299, -0.08642031],\n",
       "          [ 0.11367918,  0.02125986],\n",
       "          [ 0.03415009, -0.04828777],\n",
       "          [-0.14960594,  0.08227181],\n",
       "          [-0.10777917, -0.06837637],\n",
       "          [-0.14594026,  0.1242935 ],\n",
       "          [-0.0686616 ,  0.12820269],\n",
       "          [-0.09706645,  0.11198301],\n",
       "          [ 0.13791232,  0.09227873],\n",
       "          [-0.09291517, -0.01606669],\n",
       "          [ 0.12428611, -0.13134691],\n",
       "          [-0.07402568, -0.09248075],\n",
       "          [-0.07612282,  0.05545859],\n",
       "          [-0.02444762,  0.13631076],\n",
       "          [-0.08244818,  0.09820217],\n",
       "          [ 0.0952178 ,  0.06619597],\n",
       "          [ 0.09218622,  0.15007485],\n",
       "          [-0.09953327,  0.05792591],\n",
       "          [-0.13379271, -0.06707136],\n",
       "          [-0.01182925, -0.0559627 ],\n",
       "          [-0.11915518, -0.13703379],\n",
       "          [ 0.05084669, -0.02728878],\n",
       "          [ 0.00929368,  0.06635879],\n",
       "          [-0.04546687, -0.10438892],\n",
       "          [-0.0516797 ,  0.10609427],\n",
       "          [-0.01688221, -0.01356502],\n",
       "          [-0.0138795 , -0.02997751],\n",
       "          [ 0.07368561,  0.00714447],\n",
       "          [-0.1124005 ,  0.03860997],\n",
       "          [-0.03443117,  0.09813886],\n",
       "          [-0.00067427, -0.10917884],\n",
       "          [ 0.13485442,  0.13266493],\n",
       "          [-0.03073707,  0.13668713],\n",
       "          [-0.04945561, -0.14691265],\n",
       "          [-0.06148608, -0.0045827 ],\n",
       "          [ 0.03120962, -0.09088511],\n",
       "          [ 0.07458536,  0.11574316],\n",
       "          [ 0.08389849,  0.08504806],\n",
       "          [-0.13521406, -0.08034676],\n",
       "          [ 0.06956631,  0.0160883 ],\n",
       "          [-0.05064593,  0.10975081],\n",
       "          [-0.01832676, -0.01450115],\n",
       "          [ 0.00692189, -0.02372527],\n",
       "          [ 0.10406622,  0.02014754],\n",
       "          [ 0.00240409,  0.12291709],\n",
       "          [-0.11844958,  0.08676035],\n",
       "          [-0.08122838, -0.00766537],\n",
       "          [ 0.03489936,  0.14330457],\n",
       "          [-0.09776632, -0.06361384],\n",
       "          [ 0.00796017,  0.05468657],\n",
       "          [-0.07880459,  0.07421675],\n",
       "          [-0.09829886,  0.08555987],\n",
       "          [ 0.14717819, -0.12041624],\n",
       "          [-0.10264257, -0.15061255],\n",
       "          [-0.03919433,  0.15384561]], dtype=float32),\n",
       "   array([ 0.00785229, -0.00785403], dtype=float32)),\n",
       "  ()),\n",
       " ((),\n",
       "  (((), ((), ())), ((), ()), ()),\n",
       "  (((), ((), ())), ((), ()), ()),\n",
       "  (),\n",
       "  (),\n",
       "  ()))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentimentAnalysisModel()\n",
    "train_generator_object = train_generator(batch_size,train_pos,train_neg,Vocab)\n",
    "model.init_from_file(file_name='new_model/model.pkl.gz', weights_only=True, input_signature=shapes.signature(next(train_generator_object)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_layer = trax.AsKeras(training_loop.model)\n",
    "# # training_loop.model\n",
    "# print(keras_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = training_loop.model.weights\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Create a full Keras  model using the layer from Trax.\n",
    "# inputs = tf.keras.Input(shape=(None,), dtype='float32')\n",
    "# hidden = keras_layer(inputs)\n",
    "# # You can add other Keras layers here operating on hidden.\n",
    "# outputs = hidden\n",
    "# keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# print(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# inputs = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "# hidden = keras_layer(inputs)\n",
    "# outputs = hidden\n",
    "# keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# # print(keras_model)\n",
    "# model_file = os.path.join(output_dir, \"model_checkpoint\")\n",
    "# keras_model.save(model_file)\n",
    "\n",
    "# # Load the model from SavedModel.\n",
    "# loaded_model = tf.keras.models.load_model(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file = os.path.join(output_dir, \"model_checkpoint\")\n",
    "# model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop.save_checkpoint('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing_loop_model = trax.supervised.training.Loop.load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(training.Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
      "The shape of the tweet tensors is (16, 23) (num of examples, length of tweet tensors)\n",
      "The shape of the labels is (16,), which is the batch size.\n",
      "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
     ]
    }
   ],
   "source": [
    "# Create a generator object\n",
    "tmp_train_generator = train_generator(16, train_pos\n",
    "                    , train_neg, Vocab, loop=True\n",
    "                    , shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
      "Column 0 is the probability of a negative sentiment (class 0)\n",
      "Column 1 is the probability of a positive sentiment (class 1)\n",
      "\n",
      "View the prediction array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[-0.55772054, -0.8498305 ],\n",
       "       [-0.49686807, -0.9375993 ],\n",
       "       [-0.53799534, -0.8768718 ],\n",
       "       [-0.5661215 , -0.83868885],\n",
       "       [-0.51582736, -0.9088347 ],\n",
       "       [-0.55865717, -0.84857744],\n",
       "       [-0.5376564 , -0.8773479 ],\n",
       "       [-0.5395467 , -0.8746989 ],\n",
       "       [-0.4795301 , -0.96515065],\n",
       "       [-0.45168158, -1.0121323 ],\n",
       "       [-0.46041346, -0.9970201 ],\n",
       "       [-0.40933338, -1.0909203 ],\n",
       "       [-0.4572921 , -1.0023806 ],\n",
       "       [-0.46829343, -0.9836862 ],\n",
       "       [-0.4737927 , -0.9745457 ],\n",
       "       [-0.4611515 , -0.9957592 ]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = predictions = model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log prob -0.5577\tPos log prob -0.8498\t is positive? False\t actual 1\n",
      "Neg log prob -0.4969\tPos log prob -0.9376\t is positive? False\t actual 1\n",
      "Neg log prob -0.5380\tPos log prob -0.8769\t is positive? False\t actual 1\n",
      "Neg log prob -0.5661\tPos log prob -0.8387\t is positive? False\t actual 1\n",
      "Neg log prob -0.5158\tPos log prob -0.9088\t is positive? False\t actual 1\n",
      "Neg log prob -0.5587\tPos log prob -0.8486\t is positive? False\t actual 1\n",
      "Neg log prob -0.5377\tPos log prob -0.8773\t is positive? False\t actual 1\n",
      "Neg log prob -0.5395\tPos log prob -0.8747\t is positive? False\t actual 1\n",
      "Neg log prob -0.4795\tPos log prob -0.9652\t is positive? False\t actual 0\n",
      "Neg log prob -0.4517\tPos log prob -1.0121\t is positive? False\t actual 0\n",
      "Neg log prob -0.4604\tPos log prob -0.9970\t is positive? False\t actual 0\n",
      "Neg log prob -0.4093\tPos log prob -1.0909\t is positive? False\t actual 0\n",
      "Neg log prob -0.4573\tPos log prob -1.0024\t is positive? False\t actual 0\n",
      "Neg log prob -0.4683\tPos log prob -0.9837\t is positive? False\t actual 0\n",
      "Neg log prob -0.4738\tPos log prob -0.9745\t is positive? False\t actual 0\n",
      "Neg log prob -0.4612\tPos log prob -0.9958\t is positive? False\t actual 0\n"
     ]
    }
   ],
   "source": [
    "# turn probabilites into category predictions\n",
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"What \"\n",
      "***\n",
      "is negative.\n",
      "\n",
      "(1, 2)\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"Engineering is fine\"\n",
      "***\n",
      "is negative.\n"
     ]
    }
   ],
   "source": [
    "# this is used to predict on your own sentnece\n",
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    \n",
    "    # Batch size 1, add dimension for batch, to work with the model\n",
    "    inputs = inputs[None, :]  \n",
    "    \n",
    "    # predict with the model\n",
    "    preds_probs = model(inputs)\n",
    "    \n",
    "    print(preds_probs.shape)\n",
    "    # Turn probabilities into categories\n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    \n",
    "    sentiment = \"negative\"\n",
    "    if preds == 1:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    return preds, sentiment\n",
    "# try a positive sentence\n",
    "sentence = \"What \"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"Engineering is fine\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model on the validation set is 0.5000\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds: a tensor of shape (dim_batch, output_dim) \n",
    "        y: a tensor of shape (dim_batch,) with the true labels\n",
    "        y_weights: a n.ndarray with the a weight for each example\n",
    "    Output: \n",
    "        accuracy: a float between 0-1 \n",
    "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
    "        sum_weights (np.float32): Sum of the weights\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Create an array of booleans, \n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # the probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos = preds[:,1] > preds[:,0]\n",
    "\n",
    "    # convert the array of booleans into an array of np.int32\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    \n",
    "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
    "    correct = y == is_pos_int\n",
    "\n",
    "    # Count the sum of the weights.\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    \n",
    "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
    "    correct_float = correct.astype(np.int32)\n",
    "    \n",
    "    # Multiply each prediction with its corresponding weight.\n",
    "    weighted_correct_float = np.multiply(correct_float,y_weights)\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
    "    # numerator.\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    "\n",
    "    # Divide the number of weighted correct predictions by the sum of the\n",
    "    # weights.\n",
    "    accuracy = weighted_num_correct/sum_weights\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy, weighted_num_correct, sum_weights\n",
    "\n",
    "def test_model(generator, model, compute_accuracy=compute_accuracy):\n",
    "    '''\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model: a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    '''\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "        \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for batch in generator: \n",
    "        \n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = batch[0]\n",
    "        \n",
    "        # Retrieve the targets (actual labels) from the batch\n",
    "        targets = batch[1]\n",
    "        \n",
    "        # Retrieve the example weight.\n",
    "        example_weight = batch[2]\n",
    "\n",
    "        # Make predictions using the inputs            \n",
    "        pred = model(inputs)\n",
    "        \n",
    "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(preds=pred, y=targets, y_weights=example_weight)\n",
    "                \n",
    "        # Update the total number of correct predictions\n",
    "        # by adding the number of correct predictions from this batch\n",
    "        total_num_correct += batch_num_correct\n",
    "        \n",
    "        # Update the total number of predictions \n",
    "        # by adding the number of predictions made for the batch\n",
    "        total_num_pred += batch_num_pred\n",
    "\n",
    "    # Calculate accuracy over all examples\n",
    "    accuracy = total_num_correct/total_num_pred\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return accuracy\n",
    "# DO NOT EDIT THIS CELL\n",
    "# testing the accuracy of your model: this takes around 20 seconds\n",
    "accuracy = test_model(test_generator(16, val_pos\n",
    "                    , val_neg, Vocab, loop=False\n",
    "                    , shuffle = False), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
